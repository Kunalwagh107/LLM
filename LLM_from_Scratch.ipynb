{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_ZVCMz6k-Nhf"
      ],
      "toc_visible": true,
      "mount_file_id": "1KCZTN9PMuFBmnuQJXIcjgmzZDVKKzhiQ",
      "authorship_tag": "ABX9TyMuTEupcFcsWv9kJXHV8u3Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjoWFfd0J80f"
      },
      "outputs": [],
      "source": [
        "# comes under data preperation and sampling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how do you prepare input text?\n",
        "\n",
        "'''\n",
        "1- splitting text into individual word and subword tokens\n",
        "2- convert these token into token id\n",
        "3- encode token ID's into vector representations\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "EraAlui3KVBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens can be character, pieces of words or full words\n",
        "\n",
        "text = \"I'm learning tokenization\"\n",
        "\n",
        "character_tokens = ['I', '’', 'm', ' ', 'l', 'e'] # <--many tokens\n",
        "word_tokens = ['I’m', 'learning', 'tokenization', '!'] #<- fewer tokens, but OOV risk -> out of vocab -> chatgptnizer\n",
        "subword_tokens= ['I', '’', 'm', ' learn', 'ing', ' token', 'ization', '!'] # <- used in modern LLM's"
      ],
      "metadata": {
        "id": "yVu0d0QlsI81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Tokens"
      ],
      "metadata": {
        "id": "Gtw3_3ScLMpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *-load and read db*"
      ],
      "metadata": {
        "id": "_ZVCMz6k-Nhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/4. Data Files/LLM Dataset/the verdict.txt','r',encoding='utf-8') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "HIWIFzcg9JJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data)) # <-- length of data\n",
        "data[:99]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZXOthexz9cdv",
        "outputId": "3fea9efa-899e-4728-f525-acb2108d7e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *-splitting words*"
      ],
      "metadata": {
        "id": "EWGSSbv--nqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # <-- we will use regular expression to split the long texts\n",
        "\n",
        "text = 'Hello, this is my -- data!?\"\"'\n",
        "\n",
        "def split_func(text):\n",
        "  preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "  preprocessed = [p.strip() for p in preprocessed if p.strip()] # if p.strip() removes blank\n",
        "  return preprocessed\n"
      ],
      "metadata": {
        "id": "FoQr3nUS-VUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_func(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP_UX4C2ABIO",
        "outputId": "4e02f0bb-b55d-4f3e-8d2b-bb5421358a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'this', 'is', 'my', '--', 'data', '!', '?', '\"', '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# should we remove widespaces or not ?\n",
        "'''\n",
        "depends on application and recuirements\n",
        "-> reduces memory and computing requirements\n",
        "-> we keep in in cases where structure is required -> python indentation & spaces\n",
        " '''"
      ],
      "metadata": {
        "id": "r7LUexwrCdk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprossed = split_func(data)\n",
        "len(preprossed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVByQ9SaC2Os",
        "outputId": "79aa62ed-ae87-496b-ad54-305e478fde0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4827"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *- building vocab*"
      ],
      "metadata": {
        "id": "_YN0p0NCDm58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# small vocab -> many unknowns, slow training worse quality because long dependency"
      ],
      "metadata": {
        "id": "KgOQ5Yjlc8Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concept enumerate\n",
        "l=['Kunal', 'Bharat', 'Wagh']\n",
        "enumerate(l) # <- this is enemurate object which adds counter to each element\n",
        "for index, item in enumerate(l):\n",
        "  print(index,item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gF7hg4kH4gdU",
        "outputId": "76cb498b-a4f7-499d-dc4c-16ad0a340d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Kunal\n",
            "1 Bharat\n",
            "2 Wagh\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('name', 'Kunal'), ('age', 25), ('gender', 'male')])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding ->\n",
        "\n",
        "# unique elements and sort it and then assign the token\n",
        "preprossed # <- splitted data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7hcvtrD-Djjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_data = sorted(set(preprossed))\n",
        "len(sorted_data)\n",
        "sorted_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kcs_5o-EAej",
        "outputId": "6e316ec3-3b1a-4a08-be67-3ba5dc089d82",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'Ah',\n",
              " 'Among',\n",
              " 'And',\n",
              " 'Are',\n",
              " 'Arrt',\n",
              " 'As',\n",
              " 'At',\n",
              " 'Be',\n",
              " 'Begin',\n",
              " 'Burlington',\n",
              " 'But',\n",
              " 'By',\n",
              " 'Carlo',\n",
              " 'Chicago',\n",
              " 'Claude',\n",
              " 'Come',\n",
              " 'Croft',\n",
              " 'Destroyed',\n",
              " 'Devonshire',\n",
              " 'Don',\n",
              " 'Dubarry_',\n",
              " 'Emperors',\n",
              " 'Florence',\n",
              " 'For',\n",
              " 'Gallery',\n",
              " 'Gideon',\n",
              " 'Gisburn',\n",
              " 'Gisburns',\n",
              " 'Grafton',\n",
              " 'Greek',\n",
              " 'Grindle',\n",
              " 'Grindles',\n",
              " 'HAD',\n",
              " 'Had',\n",
              " 'Hang',\n",
              " 'Has',\n",
              " 'He',\n",
              " 'Her',\n",
              " 'Hermia',\n",
              " 'His',\n",
              " 'How',\n",
              " 'I',\n",
              " 'If',\n",
              " 'In',\n",
              " 'It',\n",
              " 'Jack',\n",
              " 'Jove',\n",
              " 'Just',\n",
              " 'Lord',\n",
              " 'Made',\n",
              " 'Miss',\n",
              " 'Money',\n",
              " 'Monte',\n",
              " 'Moon',\n",
              " 'Mr',\n",
              " 'Mrs',\n",
              " 'My',\n",
              " 'Never',\n",
              " 'No',\n",
              " 'Now',\n",
              " 'Nutley',\n",
              " 'Of',\n",
              " 'Oh',\n",
              " 'On',\n",
              " 'Once',\n",
              " 'Only',\n",
              " 'Or',\n",
              " 'Perhaps',\n",
              " 'Poor',\n",
              " 'Professional',\n",
              " 'Renaissance',\n",
              " 'Rickham',\n",
              " 'Riviera',\n",
              " 'Rome',\n",
              " 'Russian',\n",
              " 'Sevres',\n",
              " 'She',\n",
              " 'Stroud',\n",
              " 'Strouds',\n",
              " 'Suddenly',\n",
              " 'That',\n",
              " 'The',\n",
              " 'Then',\n",
              " 'There',\n",
              " 'They',\n",
              " 'This',\n",
              " 'Those',\n",
              " 'Though',\n",
              " 'Thwing',\n",
              " 'Thwings',\n",
              " 'To',\n",
              " 'Usually',\n",
              " 'Venetian',\n",
              " 'Victor',\n",
              " 'Was',\n",
              " 'We',\n",
              " 'Well',\n",
              " 'What',\n",
              " 'When',\n",
              " 'Why',\n",
              " 'Yes',\n",
              " 'You',\n",
              " '_I',\n",
              " '_am_',\n",
              " '_famille',\n",
              " '_felt_',\n",
              " '_has_',\n",
              " '_have_',\n",
              " '_jardiniere_',\n",
              " '_mine_',\n",
              " '_not_',\n",
              " '_rose',\n",
              " '_rs_',\n",
              " '_that_',\n",
              " '_the_',\n",
              " '_was_',\n",
              " '_were_',\n",
              " 'a',\n",
              " 'abdication',\n",
              " 'able',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abruptly',\n",
              " 'absolute',\n",
              " 'absorbed',\n",
              " 'absurdity',\n",
              " 'academic',\n",
              " 'accuse',\n",
              " 'accustomed',\n",
              " 'across',\n",
              " 'activity',\n",
              " 'add',\n",
              " 'added',\n",
              " 'admirers',\n",
              " 'adopted',\n",
              " 'adulation',\n",
              " 'advance',\n",
              " 'aesthetic',\n",
              " 'affect',\n",
              " 'afraid',\n",
              " 'after',\n",
              " 'afterward',\n",
              " 'again',\n",
              " 'ago',\n",
              " 'ah',\n",
              " 'air',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'always',\n",
              " 'amazement',\n",
              " 'amid',\n",
              " 'among',\n",
              " 'amplest',\n",
              " 'amusing',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'answered',\n",
              " 'any',\n",
              " 'anything',\n",
              " 'anywhere',\n",
              " 'apparent',\n",
              " 'apparently',\n",
              " 'appearance',\n",
              " 'appeared',\n",
              " 'appointed',\n",
              " 'are',\n",
              " 'arm',\n",
              " 'arms',\n",
              " 'art',\n",
              " 'articles',\n",
              " 'artist',\n",
              " 'as',\n",
              " 'aside',\n",
              " 'asked',\n",
              " 'at',\n",
              " 'atmosphere',\n",
              " 'atom',\n",
              " 'attack',\n",
              " 'attention',\n",
              " 'attitude',\n",
              " 'audacities',\n",
              " 'away',\n",
              " 'awful',\n",
              " 'axioms',\n",
              " 'azaleas',\n",
              " 'back',\n",
              " 'background',\n",
              " 'balance',\n",
              " 'balancing',\n",
              " 'balustraded',\n",
              " 'basking',\n",
              " 'bath',\n",
              " 'be',\n",
              " 'beaming',\n",
              " 'bean',\n",
              " 'bear',\n",
              " 'beard',\n",
              " 'beauty',\n",
              " 'became',\n",
              " 'because',\n",
              " 'becoming',\n",
              " 'bed',\n",
              " 'been',\n",
              " 'before',\n",
              " 'began',\n",
              " 'begun',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'believed',\n",
              " 'beneath',\n",
              " 'bespoke',\n",
              " 'better',\n",
              " 'between',\n",
              " 'big',\n",
              " 'bits',\n",
              " 'bitterness',\n",
              " 'blocked',\n",
              " 'born',\n",
              " 'borne',\n",
              " 'boudoir',\n",
              " 'brac',\n",
              " 'bravura',\n",
              " 'break',\n",
              " 'breaking',\n",
              " 'breathing',\n",
              " 'breeding',\n",
              " 'bric',\n",
              " 'briefly',\n",
              " 'brings',\n",
              " 'bronzes',\n",
              " 'brought',\n",
              " 'brown',\n",
              " 'brush',\n",
              " 'bull',\n",
              " 'business',\n",
              " 'but',\n",
              " 'buying',\n",
              " 'by',\n",
              " 'called',\n",
              " 'came',\n",
              " 'can',\n",
              " 'canvas',\n",
              " 'canvases',\n",
              " 'cards',\n",
              " 'care',\n",
              " 'career',\n",
              " 'caught',\n",
              " 'central',\n",
              " 'century',\n",
              " 'chair',\n",
              " 'chairs',\n",
              " 'chap',\n",
              " 'characteristic',\n",
              " 'charming',\n",
              " 'cheap',\n",
              " 'check',\n",
              " 'cheeks',\n",
              " 'chest',\n",
              " 'chimney',\n",
              " 'chucked',\n",
              " 'cigar',\n",
              " 'cigarette',\n",
              " 'cigars',\n",
              " 'circulation',\n",
              " 'circumstance',\n",
              " 'circus',\n",
              " 'claimed',\n",
              " 'clasping',\n",
              " 'clear',\n",
              " 'cleverer',\n",
              " 'close',\n",
              " 'closets',\n",
              " 'clown',\n",
              " 'clue',\n",
              " 'coat',\n",
              " 'collapsed',\n",
              " 'colour',\n",
              " 'come',\n",
              " 'comfortable',\n",
              " 'coming',\n",
              " 'companion',\n",
              " 'compared',\n",
              " 'complex',\n",
              " 'confident',\n",
              " 'congesting',\n",
              " 'conjugal',\n",
              " 'constraint',\n",
              " 'consummate',\n",
              " 'contended',\n",
              " 'continued',\n",
              " 'corner',\n",
              " 'corrected',\n",
              " 'cotta',\n",
              " 'could',\n",
              " 'couldn',\n",
              " 'count',\n",
              " 'countenance',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'covered',\n",
              " 'craft',\n",
              " 'cried',\n",
              " 'crossed',\n",
              " 'crowned',\n",
              " 'crumbled',\n",
              " 'cry',\n",
              " 'cured',\n",
              " 'curiosity',\n",
              " 'curious',\n",
              " 'current',\n",
              " 'curtains',\n",
              " 'd',\n",
              " 'dabble',\n",
              " 'damask',\n",
              " 'dancers',\n",
              " 'dark',\n",
              " 'dashed',\n",
              " 'day',\n",
              " 'days',\n",
              " 'dead',\n",
              " 'deadening',\n",
              " 'dear',\n",
              " 'deep',\n",
              " 'deerhound',\n",
              " 'degree',\n",
              " 'delicate',\n",
              " 'demand',\n",
              " 'denied',\n",
              " 'deploring',\n",
              " 'deprecating',\n",
              " 'deprecatingly',\n",
              " 'desire',\n",
              " 'destroyed',\n",
              " 'destruction',\n",
              " 'desultory',\n",
              " 'detail',\n",
              " 'diagnosis',\n",
              " 'did',\n",
              " 'didn',\n",
              " 'died',\n",
              " 'dim',\n",
              " 'dimmest',\n",
              " 'dingy',\n",
              " 'dining',\n",
              " 'disarming',\n",
              " 'discovery',\n",
              " 'discrimination',\n",
              " 'discussion',\n",
              " 'disdain',\n",
              " 'disdained',\n",
              " 'disease',\n",
              " 'disguised',\n",
              " 'display',\n",
              " 'dissatisfied',\n",
              " 'distinguished',\n",
              " 'distract',\n",
              " 'divert',\n",
              " 'do',\n",
              " 'doesn',\n",
              " 'doing',\n",
              " 'domestic',\n",
              " 'don',\n",
              " 'done',\n",
              " 'donkey',\n",
              " 'down',\n",
              " 'dozen',\n",
              " 'dragged',\n",
              " 'drawing',\n",
              " 'drawn',\n",
              " 'dress',\n",
              " 'drew',\n",
              " 'dropped',\n",
              " 'each',\n",
              " 'earth',\n",
              " 'ease',\n",
              " 'easel',\n",
              " 'easy',\n",
              " 'echoed',\n",
              " 'economy',\n",
              " 'effect',\n",
              " 'effects',\n",
              " 'efforts',\n",
              " 'egregious',\n",
              " 'eighteenth',\n",
              " 'elbow',\n",
              " 'elegant',\n",
              " 'else',\n",
              " 'embarrassed',\n",
              " 'enabled',\n",
              " 'end',\n",
              " 'endless',\n",
              " 'enjoy',\n",
              " 'enlightenment',\n",
              " 'enough',\n",
              " 'ensuing',\n",
              " 'equally',\n",
              " 'equanimity',\n",
              " 'escape',\n",
              " 'established',\n",
              " 'etching',\n",
              " 'even',\n",
              " 'event',\n",
              " 'ever',\n",
              " 'everlasting',\n",
              " 'every',\n",
              " 'exasperated',\n",
              " 'except',\n",
              " 'excuse',\n",
              " 'excusing',\n",
              " 'existed',\n",
              " 'expected',\n",
              " 'exquisite',\n",
              " 'exquisitely',\n",
              " 'extenuation',\n",
              " 'exterminating',\n",
              " 'extracting',\n",
              " 'eye',\n",
              " 'eyebrows',\n",
              " 'eyes',\n",
              " 'face',\n",
              " 'faces',\n",
              " 'fact',\n",
              " 'faded',\n",
              " 'failed',\n",
              " 'failure',\n",
              " 'fair',\n",
              " 'faith',\n",
              " 'false',\n",
              " 'familiar',\n",
              " 'fancy',\n",
              " 'fashionable',\n",
              " 'fate',\n",
              " 'feather',\n",
              " 'feet',\n",
              " 'fell',\n",
              " 'fellow',\n",
              " 'felt',\n",
              " 'few',\n",
              " 'fewer',\n",
              " 'finality',\n",
              " 'find',\n",
              " 'fingers',\n",
              " 'first',\n",
              " 'fit',\n",
              " 'fitting',\n",
              " 'five',\n",
              " 'flash',\n",
              " 'flashed',\n",
              " 'florid',\n",
              " 'flowers',\n",
              " 'fluently',\n",
              " 'flung',\n",
              " 'follow',\n",
              " 'followed',\n",
              " 'fond',\n",
              " 'footstep',\n",
              " 'for',\n",
              " 'forced',\n",
              " 'forcing',\n",
              " 'forehead',\n",
              " 'foreign',\n",
              " 'foreseen',\n",
              " 'forgive',\n",
              " 'forgotten',\n",
              " 'form',\n",
              " 'formed',\n",
              " 'forming',\n",
              " 'forward',\n",
              " 'fostered',\n",
              " 'found',\n",
              " 'foundations',\n",
              " 'four',\n",
              " 'fragment',\n",
              " 'fragments',\n",
              " 'frame',\n",
              " 'frames',\n",
              " 'frequently',\n",
              " 'friend',\n",
              " 'from',\n",
              " 'full',\n",
              " 'fullest',\n",
              " 'furiously',\n",
              " 'furrowed',\n",
              " 'garlanded',\n",
              " 'garlands',\n",
              " 'gave',\n",
              " 'genial',\n",
              " 'genius',\n",
              " 'gesture',\n",
              " 'get',\n",
              " 'getting',\n",
              " 'give',\n",
              " 'given',\n",
              " 'glad',\n",
              " 'glanced',\n",
              " 'glimpse',\n",
              " 'gloried',\n",
              " 'glory',\n",
              " 'go',\n",
              " 'going',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'got',\n",
              " 'grace',\n",
              " 'gradually',\n",
              " 'gray',\n",
              " 'grayish',\n",
              " 'great',\n",
              " 'greatest',\n",
              " 'greatness',\n",
              " 'grew',\n",
              " 'groping',\n",
              " 'growing',\n",
              " 'had',\n",
              " 'hadn',\n",
              " 'hair',\n",
              " 'half',\n",
              " 'hall',\n",
              " 'hand',\n",
              " 'hands',\n",
              " 'handsome',\n",
              " 'hanging',\n",
              " 'happen',\n",
              " 'happened',\n",
              " 'hard',\n",
              " 'hardly',\n",
              " 'have',\n",
              " 'haven',\n",
              " 'having',\n",
              " 'he',\n",
              " 'head',\n",
              " 'hear',\n",
              " 'heard',\n",
              " 'heart',\n",
              " 'height',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hermit',\n",
              " 'herself',\n",
              " 'hesitations',\n",
              " 'hide',\n",
              " 'high',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'hint',\n",
              " 'his',\n",
              " 'history',\n",
              " 'holding',\n",
              " 'home',\n",
              " 'honour',\n",
              " 'hooded',\n",
              " 'hostess',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'hours',\n",
              " 'house',\n",
              " 'how',\n",
              " 'humoured',\n",
              " 'hung',\n",
              " 'husband',\n",
              " 'idea',\n",
              " 'idle',\n",
              " 'idling',\n",
              " 'if',\n",
              " 'immediately',\n",
              " 'in',\n",
              " 'incense',\n",
              " 'indifferent',\n",
              " 'inevitable',\n",
              " 'inevitably',\n",
              " 'inflexible',\n",
              " 'insensible',\n",
              " 'insignificant',\n",
              " 'instinctively',\n",
              " 'instructive',\n",
              " 'interesting',\n",
              " 'into',\n",
              " 'ironic',\n",
              " 'irony',\n",
              " 'irrelevance',\n",
              " 'irrevocable',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'jealousy',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'kept',\n",
              " 'kind',\n",
              " 'knees',\n",
              " 'knew',\n",
              " 'know',\n",
              " 'known_',\n",
              " 'laid',\n",
              " 'lair',\n",
              " 'landing',\n",
              " 'language',\n",
              " 'last',\n",
              " 'late',\n",
              " 'later',\n",
              " 'latter',\n",
              " 'laugh',\n",
              " 'laughed',\n",
              " 'lay',\n",
              " 'leading',\n",
              " 'lean',\n",
              " 'learned',\n",
              " 'least',\n",
              " 'leathery',\n",
              " 'leave',\n",
              " 'led',\n",
              " 'left',\n",
              " 'leisure',\n",
              " 'lends',\n",
              " 'lent',\n",
              " 'let',\n",
              " 'lies',\n",
              " 'life',\n",
              " 'lift',\n",
              " 'lifted',\n",
              " 'light',\n",
              " 'lightly',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'likeness',\n",
              " 'line',\n",
              " 'lines',\n",
              " 'lingered',\n",
              " 'lips',\n",
              " 'lit',\n",
              " 'little',\n",
              " 'live',\n",
              " 'll',\n",
              " 'loathing',\n",
              " 'long',\n",
              " 'longed',\n",
              " 'longer',\n",
              " 'look',\n",
              " 'looked',\n",
              " 'looking',\n",
              " 'lose',\n",
              " 'loss',\n",
              " 'lounging',\n",
              " 'lovely',\n",
              " 'lucky',\n",
              " 'lump',\n",
              " 'luncheon',\n",
              " 'luxury',\n",
              " 'lying',\n",
              " 'made',\n",
              " 'make',\n",
              " 'man',\n",
              " 'manage',\n",
              " 'managed',\n",
              " 'mantel',\n",
              " 'marble',\n",
              " 'married',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meant',\n",
              " 'mechanically',\n",
              " 'mediocrity',\n",
              " 'medium',\n",
              " 'mentioned',\n",
              " 'mere',\n",
              " 'merely',\n",
              " 'met',\n",
              " 'might',\n",
              " 'mighty',\n",
              " 'millionaire',\n",
              " 'mine',\n",
              " 'minute',\n",
              " 'minutes',\n",
              " 'mirrors',\n",
              " 'modest',\n",
              " 'modesty',\n",
              " 'moment',\n",
              " 'money',\n",
              " 'monumental',\n",
              " 'mood',\n",
              " 'morbidly',\n",
              " 'more',\n",
              " 'most',\n",
              " 'mourn',\n",
              " 'mourned',\n",
              " 'moustache',\n",
              " 'moved',\n",
              " 'much',\n",
              " 'muddling',\n",
              " 'multiplied',\n",
              " 'murmur',\n",
              " 'muscles',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'mysterious',\n",
              " 'naive',\n",
              " 'near',\n",
              " 'nearly',\n",
              " 'negatived',\n",
              " 'nervous',\n",
              " 'nervousness',\n",
              " 'neutral',\n",
              " 'never',\n",
              " 'next',\n",
              " 'no',\n",
              " 'none',\n",
              " 'not',\n",
              " 'note',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nymphs',\n",
              " 'oak',\n",
              " 'obituary',\n",
              " 'object',\n",
              " 'objects',\n",
              " 'occurred',\n",
              " 'oddly',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'oh',\n",
              " 'old',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'open',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'outline',\n",
              " 'oval',\n",
              " 'over',\n",
              " 'own',\n",
              " 'packed',\n",
              " 'paid',\n",
              " 'paint',\n",
              " 'painted',\n",
              " 'painter',\n",
              " 'painting',\n",
              " 'pale',\n",
              " 'paled',\n",
              " 'palm',\n",
              " 'panel',\n",
              " 'panelled',\n",
              " 'panelling',\n",
              " 'pardonable',\n",
              " 'pardoned',\n",
              " 'part',\n",
              " 'passages',\n",
              " 'passing',\n",
              " 'past',\n",
              " 'pastels',\n",
              " 'pathos',\n",
              " 'patient',\n",
              " 'people',\n",
              " 'perceptible',\n",
              " 'perfect',\n",
              " 'persistence',\n",
              " 'persuasively',\n",
              " 'phrase',\n",
              " 'picture',\n",
              " 'pictures',\n",
              " 'piece',\n",
              " 'pines',\n",
              " 'pink',\n",
              " 'place',\n",
              " 'placed',\n",
              " 'plain',\n",
              " 'platitudes',\n",
              " 'pleased',\n",
              " 'pockets',\n",
              " 'point',\n",
              " 'poised',\n",
              " 'poor',\n",
              " 'portrait',\n",
              " 'posing',\n",
              " 'possessed',\n",
              " 'poverty',\n",
              " 'predicted',\n",
              " 'preliminary',\n",
              " 'presenting',\n",
              " 'presses',\n",
              " 'prestidigitation',\n",
              " 'pretty',\n",
              " 'previous',\n",
              " 'price',\n",
              " 'pride',\n",
              " 'princely',\n",
              " 'prism',\n",
              " 'problem',\n",
              " 'proclaiming',\n",
              " 'prodigious',\n",
              " 'profusion',\n",
              " 'protest',\n",
              " 'prove',\n",
              " 'public',\n",
              " 'purblind',\n",
              " 'purely',\n",
              " 'pushed',\n",
              " 'put',\n",
              " 'qualities',\n",
              " 'quality',\n",
              " 'queerly',\n",
              " 'question',\n",
              " 'quickly',\n",
              " 'quietly',\n",
              " 'quite',\n",
              " 'quote',\n",
              " 'rain',\n",
              " 'raised',\n",
              " 'random',\n",
              " 'rather',\n",
              " 're',\n",
              " 'real',\n",
              " 'really',\n",
              " 'reared',\n",
              " 'reason',\n",
              " 'reassurance',\n",
              " 'recovering',\n",
              " 'recreated',\n",
              " 'reflected',\n",
              " 'reflection',\n",
              " 'regrets',\n",
              " 'relatively',\n",
              " 'remained',\n",
              " 'remember',\n",
              " 'reminded',\n",
              " 'repeating',\n",
              " 'represented',\n",
              " 'reproduction',\n",
              " 'resented',\n",
              " 'resolve',\n",
              " 'resources',\n",
              " 'rest',\n",
              " 'rich',\n",
              " 'ridiculous',\n",
              " 'robbed',\n",
              " 'romantic',\n",
              " 'room',\n",
              " 'rooms',\n",
              " 'rose',\n",
              " 'rule',\n",
              " 'run',\n",
              " 's',\n",
              " 'said',\n",
              " 'same',\n",
              " 'satisfaction',\n",
              " 'savour',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'scorn',\n",
              " 'scornful',\n",
              " 'secret',\n",
              " 'see',\n",
              " 'seemed',\n",
              " 'seen',\n",
              " 'self',\n",
              " 'send',\n",
              " 'sensation',\n",
              " 'sensitive',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'set',\n",
              " 'sex',\n",
              " 'shade',\n",
              " 'shaking',\n",
              " 'shall',\n",
              " 'she',\n",
              " 'shirked',\n",
              " 'short',\n",
              " 'should',\n",
              " 'shoulder',\n",
              " 'shoulders',\n",
              " 'show',\n",
              " 'showed',\n",
              " 'showy',\n",
              " 'shrug',\n",
              " 'shrugged',\n",
              " 'sight',\n",
              " 'sign',\n",
              " 'silent',\n",
              " 'silver',\n",
              " 'similar',\n",
              " 'simpleton',\n",
              " 'simplifications',\n",
              " 'simply',\n",
              " 'since',\n",
              " 'single',\n",
              " 'sitter',\n",
              " 'sitters',\n",
              " 'sketch',\n",
              " 'skill',\n",
              " 'slight',\n",
              " 'slightly',\n",
              " 'slowly',\n",
              " 'small',\n",
              " 'smile',\n",
              " 'smiling',\n",
              " 'sneer',\n",
              " 'so',\n",
              " 'solace',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'something',\n",
              " 'spacious',\n",
              " 'spaniel',\n",
              " 'speaking',\n",
              " 'speculations',\n",
              " 'spite',\n",
              " 'splash',\n",
              " 'square',\n",
              " 'stairs',\n",
              " 'stalk',\n",
              " 'stammer',\n",
              " 'stand',\n",
              " 'standing',\n",
              " 'started',\n",
              " 'stay',\n",
              " 'still',\n",
              " 'stocked',\n",
              " 'stood',\n",
              " 'stopped',\n",
              " 'stopping',\n",
              " 'straddling',\n",
              " 'straight',\n",
              " 'strain',\n",
              " 'straining',\n",
              " 'strange',\n",
              " 'straw',\n",
              " 'stream',\n",
              " 'stroke',\n",
              " 'strokes',\n",
              " 'strolled',\n",
              " 'strongest',\n",
              " 'strongly',\n",
              " 'struck',\n",
              " 'studio',\n",
              " 'stuff',\n",
              " 'subject',\n",
              " 'substantial',\n",
              " 'suburban',\n",
              " 'such',\n",
              " 'suddenly',\n",
              " 'suffered',\n",
              " 'sugar',\n",
              " 'suggested',\n",
              " 'sunburn',\n",
              " 'sunburnt',\n",
              " 'sunlit',\n",
              " 'superb',\n",
              " 'sure',\n",
              " 'surest',\n",
              " 'surface',\n",
              " 'surprise',\n",
              " 'surprised',\n",
              " 'surrounded',\n",
              " 'suspected',\n",
              " 'sweetly',\n",
              " 'sweetness',\n",
              " 'swelling',\n",
              " 'swept',\n",
              " 'swum',\n",
              " 't',\n",
              " 'table',\n",
              " 'take',\n",
              " 'taken',\n",
              " 'talking',\n",
              " 'tea',\n",
              " 'tears',\n",
              " 'technicalities',\n",
              " 'technique',\n",
              " 'tell',\n",
              " 'tells',\n",
              " 'tempting',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(sorted_data)} # use enemurate\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bKKzWAsUEOS8",
        "outputId": "76374281-c552-4ba2-bac4-83411a1ad8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '\"': 1,\n",
              " \"'\": 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'Ah': 12,\n",
              " 'Among': 13,\n",
              " 'And': 14,\n",
              " 'Are': 15,\n",
              " 'Arrt': 16,\n",
              " 'As': 17,\n",
              " 'At': 18,\n",
              " 'Be': 19,\n",
              " 'Begin': 20,\n",
              " 'Burlington': 21,\n",
              " 'But': 22,\n",
              " 'By': 23,\n",
              " 'Carlo': 24,\n",
              " 'Chicago': 25,\n",
              " 'Claude': 26,\n",
              " 'Come': 27,\n",
              " 'Croft': 28,\n",
              " 'Destroyed': 29,\n",
              " 'Devonshire': 30,\n",
              " 'Don': 31,\n",
              " 'Dubarry_': 32,\n",
              " 'Emperors': 33,\n",
              " 'Florence': 34,\n",
              " 'For': 35,\n",
              " 'Gallery': 36,\n",
              " 'Gideon': 37,\n",
              " 'Gisburn': 38,\n",
              " 'Gisburns': 39,\n",
              " 'Grafton': 40,\n",
              " 'Greek': 41,\n",
              " 'Grindle': 42,\n",
              " 'Grindles': 43,\n",
              " 'HAD': 44,\n",
              " 'Had': 45,\n",
              " 'Hang': 46,\n",
              " 'Has': 47,\n",
              " 'He': 48,\n",
              " 'Her': 49,\n",
              " 'Hermia': 50,\n",
              " 'His': 51,\n",
              " 'How': 52,\n",
              " 'I': 53,\n",
              " 'If': 54,\n",
              " 'In': 55,\n",
              " 'It': 56,\n",
              " 'Jack': 57,\n",
              " 'Jove': 58,\n",
              " 'Just': 59,\n",
              " 'Lord': 60,\n",
              " 'Made': 61,\n",
              " 'Miss': 62,\n",
              " 'Money': 63,\n",
              " 'Monte': 64,\n",
              " 'Moon': 65,\n",
              " 'Mr': 66,\n",
              " 'Mrs': 67,\n",
              " 'My': 68,\n",
              " 'Never': 69,\n",
              " 'No': 70,\n",
              " 'Now': 71,\n",
              " 'Nutley': 72,\n",
              " 'Of': 73,\n",
              " 'Oh': 74,\n",
              " 'On': 75,\n",
              " 'Once': 76,\n",
              " 'Only': 77,\n",
              " 'Or': 78,\n",
              " 'Perhaps': 79,\n",
              " 'Poor': 80,\n",
              " 'Professional': 81,\n",
              " 'Renaissance': 82,\n",
              " 'Rickham': 83,\n",
              " 'Riviera': 84,\n",
              " 'Rome': 85,\n",
              " 'Russian': 86,\n",
              " 'Sevres': 87,\n",
              " 'She': 88,\n",
              " 'Stroud': 89,\n",
              " 'Strouds': 90,\n",
              " 'Suddenly': 91,\n",
              " 'That': 92,\n",
              " 'The': 93,\n",
              " 'Then': 94,\n",
              " 'There': 95,\n",
              " 'They': 96,\n",
              " 'This': 97,\n",
              " 'Those': 98,\n",
              " 'Though': 99,\n",
              " 'Thwing': 100,\n",
              " 'Thwings': 101,\n",
              " 'To': 102,\n",
              " 'Usually': 103,\n",
              " 'Venetian': 104,\n",
              " 'Victor': 105,\n",
              " 'Was': 106,\n",
              " 'We': 107,\n",
              " 'Well': 108,\n",
              " 'What': 109,\n",
              " 'When': 110,\n",
              " 'Why': 111,\n",
              " 'Yes': 112,\n",
              " 'You': 113,\n",
              " '_I': 114,\n",
              " '_am_': 115,\n",
              " '_famille': 116,\n",
              " '_felt_': 117,\n",
              " '_has_': 118,\n",
              " '_have_': 119,\n",
              " '_jardiniere_': 120,\n",
              " '_mine_': 121,\n",
              " '_not_': 122,\n",
              " '_rose': 123,\n",
              " '_rs_': 124,\n",
              " '_that_': 125,\n",
              " '_the_': 126,\n",
              " '_was_': 127,\n",
              " '_were_': 128,\n",
              " 'a': 129,\n",
              " 'abdication': 130,\n",
              " 'able': 131,\n",
              " 'about': 132,\n",
              " 'above': 133,\n",
              " 'abruptly': 134,\n",
              " 'absolute': 135,\n",
              " 'absorbed': 136,\n",
              " 'absurdity': 137,\n",
              " 'academic': 138,\n",
              " 'accuse': 139,\n",
              " 'accustomed': 140,\n",
              " 'across': 141,\n",
              " 'activity': 142,\n",
              " 'add': 143,\n",
              " 'added': 144,\n",
              " 'admirers': 145,\n",
              " 'adopted': 146,\n",
              " 'adulation': 147,\n",
              " 'advance': 148,\n",
              " 'aesthetic': 149,\n",
              " 'affect': 150,\n",
              " 'afraid': 151,\n",
              " 'after': 152,\n",
              " 'afterward': 153,\n",
              " 'again': 154,\n",
              " 'ago': 155,\n",
              " 'ah': 156,\n",
              " 'air': 157,\n",
              " 'alive': 158,\n",
              " 'all': 159,\n",
              " 'almost': 160,\n",
              " 'alone': 161,\n",
              " 'along': 162,\n",
              " 'always': 163,\n",
              " 'amazement': 164,\n",
              " 'amid': 165,\n",
              " 'among': 166,\n",
              " 'amplest': 167,\n",
              " 'amusing': 168,\n",
              " 'an': 169,\n",
              " 'and': 170,\n",
              " 'another': 171,\n",
              " 'answer': 172,\n",
              " 'answered': 173,\n",
              " 'any': 174,\n",
              " 'anything': 175,\n",
              " 'anywhere': 176,\n",
              " 'apparent': 177,\n",
              " 'apparently': 178,\n",
              " 'appearance': 179,\n",
              " 'appeared': 180,\n",
              " 'appointed': 181,\n",
              " 'are': 182,\n",
              " 'arm': 183,\n",
              " 'arms': 184,\n",
              " 'art': 185,\n",
              " 'articles': 186,\n",
              " 'artist': 187,\n",
              " 'as': 188,\n",
              " 'aside': 189,\n",
              " 'asked': 190,\n",
              " 'at': 191,\n",
              " 'atmosphere': 192,\n",
              " 'atom': 193,\n",
              " 'attack': 194,\n",
              " 'attention': 195,\n",
              " 'attitude': 196,\n",
              " 'audacities': 197,\n",
              " 'away': 198,\n",
              " 'awful': 199,\n",
              " 'axioms': 200,\n",
              " 'azaleas': 201,\n",
              " 'back': 202,\n",
              " 'background': 203,\n",
              " 'balance': 204,\n",
              " 'balancing': 205,\n",
              " 'balustraded': 206,\n",
              " 'basking': 207,\n",
              " 'bath': 208,\n",
              " 'be': 209,\n",
              " 'beaming': 210,\n",
              " 'bean': 211,\n",
              " 'bear': 212,\n",
              " 'beard': 213,\n",
              " 'beauty': 214,\n",
              " 'became': 215,\n",
              " 'because': 216,\n",
              " 'becoming': 217,\n",
              " 'bed': 218,\n",
              " 'been': 219,\n",
              " 'before': 220,\n",
              " 'began': 221,\n",
              " 'begun': 222,\n",
              " 'behind': 223,\n",
              " 'being': 224,\n",
              " 'believed': 225,\n",
              " 'beneath': 226,\n",
              " 'bespoke': 227,\n",
              " 'better': 228,\n",
              " 'between': 229,\n",
              " 'big': 230,\n",
              " 'bits': 231,\n",
              " 'bitterness': 232,\n",
              " 'blocked': 233,\n",
              " 'born': 234,\n",
              " 'borne': 235,\n",
              " 'boudoir': 236,\n",
              " 'brac': 237,\n",
              " 'bravura': 238,\n",
              " 'break': 239,\n",
              " 'breaking': 240,\n",
              " 'breathing': 241,\n",
              " 'breeding': 242,\n",
              " 'bric': 243,\n",
              " 'briefly': 244,\n",
              " 'brings': 245,\n",
              " 'bronzes': 246,\n",
              " 'brought': 247,\n",
              " 'brown': 248,\n",
              " 'brush': 249,\n",
              " 'bull': 250,\n",
              " 'business': 251,\n",
              " 'but': 252,\n",
              " 'buying': 253,\n",
              " 'by': 254,\n",
              " 'called': 255,\n",
              " 'came': 256,\n",
              " 'can': 257,\n",
              " 'canvas': 258,\n",
              " 'canvases': 259,\n",
              " 'cards': 260,\n",
              " 'care': 261,\n",
              " 'career': 262,\n",
              " 'caught': 263,\n",
              " 'central': 264,\n",
              " 'century': 265,\n",
              " 'chair': 266,\n",
              " 'chairs': 267,\n",
              " 'chap': 268,\n",
              " 'characteristic': 269,\n",
              " 'charming': 270,\n",
              " 'cheap': 271,\n",
              " 'check': 272,\n",
              " 'cheeks': 273,\n",
              " 'chest': 274,\n",
              " 'chimney': 275,\n",
              " 'chucked': 276,\n",
              " 'cigar': 277,\n",
              " 'cigarette': 278,\n",
              " 'cigars': 279,\n",
              " 'circulation': 280,\n",
              " 'circumstance': 281,\n",
              " 'circus': 282,\n",
              " 'claimed': 283,\n",
              " 'clasping': 284,\n",
              " 'clear': 285,\n",
              " 'cleverer': 286,\n",
              " 'close': 287,\n",
              " 'closets': 288,\n",
              " 'clown': 289,\n",
              " 'clue': 290,\n",
              " 'coat': 291,\n",
              " 'collapsed': 292,\n",
              " 'colour': 293,\n",
              " 'come': 294,\n",
              " 'comfortable': 295,\n",
              " 'coming': 296,\n",
              " 'companion': 297,\n",
              " 'compared': 298,\n",
              " 'complex': 299,\n",
              " 'confident': 300,\n",
              " 'congesting': 301,\n",
              " 'conjugal': 302,\n",
              " 'constraint': 303,\n",
              " 'consummate': 304,\n",
              " 'contended': 305,\n",
              " 'continued': 306,\n",
              " 'corner': 307,\n",
              " 'corrected': 308,\n",
              " 'cotta': 309,\n",
              " 'could': 310,\n",
              " 'couldn': 311,\n",
              " 'count': 312,\n",
              " 'countenance': 313,\n",
              " 'couple': 314,\n",
              " 'course': 315,\n",
              " 'covered': 316,\n",
              " 'craft': 317,\n",
              " 'cried': 318,\n",
              " 'crossed': 319,\n",
              " 'crowned': 320,\n",
              " 'crumbled': 321,\n",
              " 'cry': 322,\n",
              " 'cured': 323,\n",
              " 'curiosity': 324,\n",
              " 'curious': 325,\n",
              " 'current': 326,\n",
              " 'curtains': 327,\n",
              " 'd': 328,\n",
              " 'dabble': 329,\n",
              " 'damask': 330,\n",
              " 'dancers': 331,\n",
              " 'dark': 332,\n",
              " 'dashed': 333,\n",
              " 'day': 334,\n",
              " 'days': 335,\n",
              " 'dead': 336,\n",
              " 'deadening': 337,\n",
              " 'dear': 338,\n",
              " 'deep': 339,\n",
              " 'deerhound': 340,\n",
              " 'degree': 341,\n",
              " 'delicate': 342,\n",
              " 'demand': 343,\n",
              " 'denied': 344,\n",
              " 'deploring': 345,\n",
              " 'deprecating': 346,\n",
              " 'deprecatingly': 347,\n",
              " 'desire': 348,\n",
              " 'destroyed': 349,\n",
              " 'destruction': 350,\n",
              " 'desultory': 351,\n",
              " 'detail': 352,\n",
              " 'diagnosis': 353,\n",
              " 'did': 354,\n",
              " 'didn': 355,\n",
              " 'died': 356,\n",
              " 'dim': 357,\n",
              " 'dimmest': 358,\n",
              " 'dingy': 359,\n",
              " 'dining': 360,\n",
              " 'disarming': 361,\n",
              " 'discovery': 362,\n",
              " 'discrimination': 363,\n",
              " 'discussion': 364,\n",
              " 'disdain': 365,\n",
              " 'disdained': 366,\n",
              " 'disease': 367,\n",
              " 'disguised': 368,\n",
              " 'display': 369,\n",
              " 'dissatisfied': 370,\n",
              " 'distinguished': 371,\n",
              " 'distract': 372,\n",
              " 'divert': 373,\n",
              " 'do': 374,\n",
              " 'doesn': 375,\n",
              " 'doing': 376,\n",
              " 'domestic': 377,\n",
              " 'don': 378,\n",
              " 'done': 379,\n",
              " 'donkey': 380,\n",
              " 'down': 381,\n",
              " 'dozen': 382,\n",
              " 'dragged': 383,\n",
              " 'drawing': 384,\n",
              " 'drawn': 385,\n",
              " 'dress': 386,\n",
              " 'drew': 387,\n",
              " 'dropped': 388,\n",
              " 'each': 389,\n",
              " 'earth': 390,\n",
              " 'ease': 391,\n",
              " 'easel': 392,\n",
              " 'easy': 393,\n",
              " 'echoed': 394,\n",
              " 'economy': 395,\n",
              " 'effect': 396,\n",
              " 'effects': 397,\n",
              " 'efforts': 398,\n",
              " 'egregious': 399,\n",
              " 'eighteenth': 400,\n",
              " 'elbow': 401,\n",
              " 'elegant': 402,\n",
              " 'else': 403,\n",
              " 'embarrassed': 404,\n",
              " 'enabled': 405,\n",
              " 'end': 406,\n",
              " 'endless': 407,\n",
              " 'enjoy': 408,\n",
              " 'enlightenment': 409,\n",
              " 'enough': 410,\n",
              " 'ensuing': 411,\n",
              " 'equally': 412,\n",
              " 'equanimity': 413,\n",
              " 'escape': 414,\n",
              " 'established': 415,\n",
              " 'etching': 416,\n",
              " 'even': 417,\n",
              " 'event': 418,\n",
              " 'ever': 419,\n",
              " 'everlasting': 420,\n",
              " 'every': 421,\n",
              " 'exasperated': 422,\n",
              " 'except': 423,\n",
              " 'excuse': 424,\n",
              " 'excusing': 425,\n",
              " 'existed': 426,\n",
              " 'expected': 427,\n",
              " 'exquisite': 428,\n",
              " 'exquisitely': 429,\n",
              " 'extenuation': 430,\n",
              " 'exterminating': 431,\n",
              " 'extracting': 432,\n",
              " 'eye': 433,\n",
              " 'eyebrows': 434,\n",
              " 'eyes': 435,\n",
              " 'face': 436,\n",
              " 'faces': 437,\n",
              " 'fact': 438,\n",
              " 'faded': 439,\n",
              " 'failed': 440,\n",
              " 'failure': 441,\n",
              " 'fair': 442,\n",
              " 'faith': 443,\n",
              " 'false': 444,\n",
              " 'familiar': 445,\n",
              " 'fancy': 446,\n",
              " 'fashionable': 447,\n",
              " 'fate': 448,\n",
              " 'feather': 449,\n",
              " 'feet': 450,\n",
              " 'fell': 451,\n",
              " 'fellow': 452,\n",
              " 'felt': 453,\n",
              " 'few': 454,\n",
              " 'fewer': 455,\n",
              " 'finality': 456,\n",
              " 'find': 457,\n",
              " 'fingers': 458,\n",
              " 'first': 459,\n",
              " 'fit': 460,\n",
              " 'fitting': 461,\n",
              " 'five': 462,\n",
              " 'flash': 463,\n",
              " 'flashed': 464,\n",
              " 'florid': 465,\n",
              " 'flowers': 466,\n",
              " 'fluently': 467,\n",
              " 'flung': 468,\n",
              " 'follow': 469,\n",
              " 'followed': 470,\n",
              " 'fond': 471,\n",
              " 'footstep': 472,\n",
              " 'for': 473,\n",
              " 'forced': 474,\n",
              " 'forcing': 475,\n",
              " 'forehead': 476,\n",
              " 'foreign': 477,\n",
              " 'foreseen': 478,\n",
              " 'forgive': 479,\n",
              " 'forgotten': 480,\n",
              " 'form': 481,\n",
              " 'formed': 482,\n",
              " 'forming': 483,\n",
              " 'forward': 484,\n",
              " 'fostered': 485,\n",
              " 'found': 486,\n",
              " 'foundations': 487,\n",
              " 'four': 488,\n",
              " 'fragment': 489,\n",
              " 'fragments': 490,\n",
              " 'frame': 491,\n",
              " 'frames': 492,\n",
              " 'frequently': 493,\n",
              " 'friend': 494,\n",
              " 'from': 495,\n",
              " 'full': 496,\n",
              " 'fullest': 497,\n",
              " 'furiously': 498,\n",
              " 'furrowed': 499,\n",
              " 'garlanded': 500,\n",
              " 'garlands': 501,\n",
              " 'gave': 502,\n",
              " 'genial': 503,\n",
              " 'genius': 504,\n",
              " 'gesture': 505,\n",
              " 'get': 506,\n",
              " 'getting': 507,\n",
              " 'give': 508,\n",
              " 'given': 509,\n",
              " 'glad': 510,\n",
              " 'glanced': 511,\n",
              " 'glimpse': 512,\n",
              " 'gloried': 513,\n",
              " 'glory': 514,\n",
              " 'go': 515,\n",
              " 'going': 516,\n",
              " 'gone': 517,\n",
              " 'good': 518,\n",
              " 'got': 519,\n",
              " 'grace': 520,\n",
              " 'gradually': 521,\n",
              " 'gray': 522,\n",
              " 'grayish': 523,\n",
              " 'great': 524,\n",
              " 'greatest': 525,\n",
              " 'greatness': 526,\n",
              " 'grew': 527,\n",
              " 'groping': 528,\n",
              " 'growing': 529,\n",
              " 'had': 530,\n",
              " 'hadn': 531,\n",
              " 'hair': 532,\n",
              " 'half': 533,\n",
              " 'hall': 534,\n",
              " 'hand': 535,\n",
              " 'hands': 536,\n",
              " 'handsome': 537,\n",
              " 'hanging': 538,\n",
              " 'happen': 539,\n",
              " 'happened': 540,\n",
              " 'hard': 541,\n",
              " 'hardly': 542,\n",
              " 'have': 543,\n",
              " 'haven': 544,\n",
              " 'having': 545,\n",
              " 'he': 546,\n",
              " 'head': 547,\n",
              " 'hear': 548,\n",
              " 'heard': 549,\n",
              " 'heart': 550,\n",
              " 'height': 551,\n",
              " 'her': 552,\n",
              " 'here': 553,\n",
              " 'hermit': 554,\n",
              " 'herself': 555,\n",
              " 'hesitations': 556,\n",
              " 'hide': 557,\n",
              " 'high': 558,\n",
              " 'him': 559,\n",
              " 'himself': 560,\n",
              " 'hint': 561,\n",
              " 'his': 562,\n",
              " 'history': 563,\n",
              " 'holding': 564,\n",
              " 'home': 565,\n",
              " 'honour': 566,\n",
              " 'hooded': 567,\n",
              " 'hostess': 568,\n",
              " 'hot': 569,\n",
              " 'hour': 570,\n",
              " 'hours': 571,\n",
              " 'house': 572,\n",
              " 'how': 573,\n",
              " 'humoured': 574,\n",
              " 'hung': 575,\n",
              " 'husband': 576,\n",
              " 'idea': 577,\n",
              " 'idle': 578,\n",
              " 'idling': 579,\n",
              " 'if': 580,\n",
              " 'immediately': 581,\n",
              " 'in': 582,\n",
              " 'incense': 583,\n",
              " 'indifferent': 584,\n",
              " 'inevitable': 585,\n",
              " 'inevitably': 586,\n",
              " 'inflexible': 587,\n",
              " 'insensible': 588,\n",
              " 'insignificant': 589,\n",
              " 'instinctively': 590,\n",
              " 'instructive': 591,\n",
              " 'interesting': 592,\n",
              " 'into': 593,\n",
              " 'ironic': 594,\n",
              " 'irony': 595,\n",
              " 'irrelevance': 596,\n",
              " 'irrevocable': 597,\n",
              " 'is': 598,\n",
              " 'it': 599,\n",
              " 'its': 600,\n",
              " 'itself': 601,\n",
              " 'jealousy': 602,\n",
              " 'just': 603,\n",
              " 'keep': 604,\n",
              " 'kept': 605,\n",
              " 'kind': 606,\n",
              " 'knees': 607,\n",
              " 'knew': 608,\n",
              " 'know': 609,\n",
              " 'known_': 610,\n",
              " 'laid': 611,\n",
              " 'lair': 612,\n",
              " 'landing': 613,\n",
              " 'language': 614,\n",
              " 'last': 615,\n",
              " 'late': 616,\n",
              " 'later': 617,\n",
              " 'latter': 618,\n",
              " 'laugh': 619,\n",
              " 'laughed': 620,\n",
              " 'lay': 621,\n",
              " 'leading': 622,\n",
              " 'lean': 623,\n",
              " 'learned': 624,\n",
              " 'least': 625,\n",
              " 'leathery': 626,\n",
              " 'leave': 627,\n",
              " 'led': 628,\n",
              " 'left': 629,\n",
              " 'leisure': 630,\n",
              " 'lends': 631,\n",
              " 'lent': 632,\n",
              " 'let': 633,\n",
              " 'lies': 634,\n",
              " 'life': 635,\n",
              " 'lift': 636,\n",
              " 'lifted': 637,\n",
              " 'light': 638,\n",
              " 'lightly': 639,\n",
              " 'like': 640,\n",
              " 'liked': 641,\n",
              " 'likeness': 642,\n",
              " 'line': 643,\n",
              " 'lines': 644,\n",
              " 'lingered': 645,\n",
              " 'lips': 646,\n",
              " 'lit': 647,\n",
              " 'little': 648,\n",
              " 'live': 649,\n",
              " 'll': 650,\n",
              " 'loathing': 651,\n",
              " 'long': 652,\n",
              " 'longed': 653,\n",
              " 'longer': 654,\n",
              " 'look': 655,\n",
              " 'looked': 656,\n",
              " 'looking': 657,\n",
              " 'lose': 658,\n",
              " 'loss': 659,\n",
              " 'lounging': 660,\n",
              " 'lovely': 661,\n",
              " 'lucky': 662,\n",
              " 'lump': 663,\n",
              " 'luncheon': 664,\n",
              " 'luxury': 665,\n",
              " 'lying': 666,\n",
              " 'made': 667,\n",
              " 'make': 668,\n",
              " 'man': 669,\n",
              " 'manage': 670,\n",
              " 'managed': 671,\n",
              " 'mantel': 672,\n",
              " 'marble': 673,\n",
              " 'married': 674,\n",
              " 'may': 675,\n",
              " 'me': 676,\n",
              " 'meant': 677,\n",
              " 'mechanically': 678,\n",
              " 'mediocrity': 679,\n",
              " 'medium': 680,\n",
              " 'mentioned': 681,\n",
              " 'mere': 682,\n",
              " 'merely': 683,\n",
              " 'met': 684,\n",
              " 'might': 685,\n",
              " 'mighty': 686,\n",
              " 'millionaire': 687,\n",
              " 'mine': 688,\n",
              " 'minute': 689,\n",
              " 'minutes': 690,\n",
              " 'mirrors': 691,\n",
              " 'modest': 692,\n",
              " 'modesty': 693,\n",
              " 'moment': 694,\n",
              " 'money': 695,\n",
              " 'monumental': 696,\n",
              " 'mood': 697,\n",
              " 'morbidly': 698,\n",
              " 'more': 699,\n",
              " 'most': 700,\n",
              " 'mourn': 701,\n",
              " 'mourned': 702,\n",
              " 'moustache': 703,\n",
              " 'moved': 704,\n",
              " 'much': 705,\n",
              " 'muddling': 706,\n",
              " 'multiplied': 707,\n",
              " 'murmur': 708,\n",
              " 'muscles': 709,\n",
              " 'must': 710,\n",
              " 'my': 711,\n",
              " 'myself': 712,\n",
              " 'mysterious': 713,\n",
              " 'naive': 714,\n",
              " 'near': 715,\n",
              " 'nearly': 716,\n",
              " 'negatived': 717,\n",
              " 'nervous': 718,\n",
              " 'nervousness': 719,\n",
              " 'neutral': 720,\n",
              " 'never': 721,\n",
              " 'next': 722,\n",
              " 'no': 723,\n",
              " 'none': 724,\n",
              " 'not': 725,\n",
              " 'note': 726,\n",
              " 'nothing': 727,\n",
              " 'now': 728,\n",
              " 'nymphs': 729,\n",
              " 'oak': 730,\n",
              " 'obituary': 731,\n",
              " 'object': 732,\n",
              " 'objects': 733,\n",
              " 'occurred': 734,\n",
              " 'oddly': 735,\n",
              " 'of': 736,\n",
              " 'off': 737,\n",
              " 'often': 738,\n",
              " 'oh': 739,\n",
              " 'old': 740,\n",
              " 'on': 741,\n",
              " 'once': 742,\n",
              " 'one': 743,\n",
              " 'ones': 744,\n",
              " 'only': 745,\n",
              " 'onto': 746,\n",
              " 'open': 747,\n",
              " 'or': 748,\n",
              " 'other': 749,\n",
              " 'our': 750,\n",
              " 'ourselves': 751,\n",
              " 'out': 752,\n",
              " 'outline': 753,\n",
              " 'oval': 754,\n",
              " 'over': 755,\n",
              " 'own': 756,\n",
              " 'packed': 757,\n",
              " 'paid': 758,\n",
              " 'paint': 759,\n",
              " 'painted': 760,\n",
              " 'painter': 761,\n",
              " 'painting': 762,\n",
              " 'pale': 763,\n",
              " 'paled': 764,\n",
              " 'palm': 765,\n",
              " 'panel': 766,\n",
              " 'panelled': 767,\n",
              " 'panelling': 768,\n",
              " 'pardonable': 769,\n",
              " 'pardoned': 770,\n",
              " 'part': 771,\n",
              " 'passages': 772,\n",
              " 'passing': 773,\n",
              " 'past': 774,\n",
              " 'pastels': 775,\n",
              " 'pathos': 776,\n",
              " 'patient': 777,\n",
              " 'people': 778,\n",
              " 'perceptible': 779,\n",
              " 'perfect': 780,\n",
              " 'persistence': 781,\n",
              " 'persuasively': 782,\n",
              " 'phrase': 783,\n",
              " 'picture': 784,\n",
              " 'pictures': 785,\n",
              " 'piece': 786,\n",
              " 'pines': 787,\n",
              " 'pink': 788,\n",
              " 'place': 789,\n",
              " 'placed': 790,\n",
              " 'plain': 791,\n",
              " 'platitudes': 792,\n",
              " 'pleased': 793,\n",
              " 'pockets': 794,\n",
              " 'point': 795,\n",
              " 'poised': 796,\n",
              " 'poor': 797,\n",
              " 'portrait': 798,\n",
              " 'posing': 799,\n",
              " 'possessed': 800,\n",
              " 'poverty': 801,\n",
              " 'predicted': 802,\n",
              " 'preliminary': 803,\n",
              " 'presenting': 804,\n",
              " 'presses': 805,\n",
              " 'prestidigitation': 806,\n",
              " 'pretty': 807,\n",
              " 'previous': 808,\n",
              " 'price': 809,\n",
              " 'pride': 810,\n",
              " 'princely': 811,\n",
              " 'prism': 812,\n",
              " 'problem': 813,\n",
              " 'proclaiming': 814,\n",
              " 'prodigious': 815,\n",
              " 'profusion': 816,\n",
              " 'protest': 817,\n",
              " 'prove': 818,\n",
              " 'public': 819,\n",
              " 'purblind': 820,\n",
              " 'purely': 821,\n",
              " 'pushed': 822,\n",
              " 'put': 823,\n",
              " 'qualities': 824,\n",
              " 'quality': 825,\n",
              " 'queerly': 826,\n",
              " 'question': 827,\n",
              " 'quickly': 828,\n",
              " 'quietly': 829,\n",
              " 'quite': 830,\n",
              " 'quote': 831,\n",
              " 'rain': 832,\n",
              " 'raised': 833,\n",
              " 'random': 834,\n",
              " 'rather': 835,\n",
              " 're': 836,\n",
              " 'real': 837,\n",
              " 'really': 838,\n",
              " 'reared': 839,\n",
              " 'reason': 840,\n",
              " 'reassurance': 841,\n",
              " 'recovering': 842,\n",
              " 'recreated': 843,\n",
              " 'reflected': 844,\n",
              " 'reflection': 845,\n",
              " 'regrets': 846,\n",
              " 'relatively': 847,\n",
              " 'remained': 848,\n",
              " 'remember': 849,\n",
              " 'reminded': 850,\n",
              " 'repeating': 851,\n",
              " 'represented': 852,\n",
              " 'reproduction': 853,\n",
              " 'resented': 854,\n",
              " 'resolve': 855,\n",
              " 'resources': 856,\n",
              " 'rest': 857,\n",
              " 'rich': 858,\n",
              " 'ridiculous': 859,\n",
              " 'robbed': 860,\n",
              " 'romantic': 861,\n",
              " 'room': 862,\n",
              " 'rooms': 863,\n",
              " 'rose': 864,\n",
              " 'rule': 865,\n",
              " 'run': 866,\n",
              " 's': 867,\n",
              " 'said': 868,\n",
              " 'same': 869,\n",
              " 'satisfaction': 870,\n",
              " 'savour': 871,\n",
              " 'saw': 872,\n",
              " 'say': 873,\n",
              " 'saying': 874,\n",
              " 'says': 875,\n",
              " 'scorn': 876,\n",
              " 'scornful': 877,\n",
              " 'secret': 878,\n",
              " 'see': 879,\n",
              " 'seemed': 880,\n",
              " 'seen': 881,\n",
              " 'self': 882,\n",
              " 'send': 883,\n",
              " 'sensation': 884,\n",
              " 'sensitive': 885,\n",
              " 'sent': 886,\n",
              " 'serious': 887,\n",
              " 'set': 888,\n",
              " 'sex': 889,\n",
              " 'shade': 890,\n",
              " 'shaking': 891,\n",
              " 'shall': 892,\n",
              " 'she': 893,\n",
              " 'shirked': 894,\n",
              " 'short': 895,\n",
              " 'should': 896,\n",
              " 'shoulder': 897,\n",
              " 'shoulders': 898,\n",
              " 'show': 899,\n",
              " 'showed': 900,\n",
              " 'showy': 901,\n",
              " 'shrug': 902,\n",
              " 'shrugged': 903,\n",
              " 'sight': 904,\n",
              " 'sign': 905,\n",
              " 'silent': 906,\n",
              " 'silver': 907,\n",
              " 'similar': 908,\n",
              " 'simpleton': 909,\n",
              " 'simplifications': 910,\n",
              " 'simply': 911,\n",
              " 'since': 912,\n",
              " 'single': 913,\n",
              " 'sitter': 914,\n",
              " 'sitters': 915,\n",
              " 'sketch': 916,\n",
              " 'skill': 917,\n",
              " 'slight': 918,\n",
              " 'slightly': 919,\n",
              " 'slowly': 920,\n",
              " 'small': 921,\n",
              " 'smile': 922,\n",
              " 'smiling': 923,\n",
              " 'sneer': 924,\n",
              " 'so': 925,\n",
              " 'solace': 926,\n",
              " 'some': 927,\n",
              " 'somebody': 928,\n",
              " 'something': 929,\n",
              " 'spacious': 930,\n",
              " 'spaniel': 931,\n",
              " 'speaking': 932,\n",
              " 'speculations': 933,\n",
              " 'spite': 934,\n",
              " 'splash': 935,\n",
              " 'square': 936,\n",
              " 'stairs': 937,\n",
              " 'stalk': 938,\n",
              " 'stammer': 939,\n",
              " 'stand': 940,\n",
              " 'standing': 941,\n",
              " 'started': 942,\n",
              " 'stay': 943,\n",
              " 'still': 944,\n",
              " 'stocked': 945,\n",
              " 'stood': 946,\n",
              " 'stopped': 947,\n",
              " 'stopping': 948,\n",
              " 'straddling': 949,\n",
              " 'straight': 950,\n",
              " 'strain': 951,\n",
              " 'straining': 952,\n",
              " 'strange': 953,\n",
              " 'straw': 954,\n",
              " 'stream': 955,\n",
              " 'stroke': 956,\n",
              " 'strokes': 957,\n",
              " 'strolled': 958,\n",
              " 'strongest': 959,\n",
              " 'strongly': 960,\n",
              " 'struck': 961,\n",
              " 'studio': 962,\n",
              " 'stuff': 963,\n",
              " 'subject': 964,\n",
              " 'substantial': 965,\n",
              " 'suburban': 966,\n",
              " 'such': 967,\n",
              " 'suddenly': 968,\n",
              " 'suffered': 969,\n",
              " 'sugar': 970,\n",
              " 'suggested': 971,\n",
              " 'sunburn': 972,\n",
              " 'sunburnt': 973,\n",
              " 'sunlit': 974,\n",
              " 'superb': 975,\n",
              " 'sure': 976,\n",
              " 'surest': 977,\n",
              " 'surface': 978,\n",
              " 'surprise': 979,\n",
              " 'surprised': 980,\n",
              " 'surrounded': 981,\n",
              " 'suspected': 982,\n",
              " 'sweetly': 983,\n",
              " 'sweetness': 984,\n",
              " 'swelling': 985,\n",
              " 'swept': 986,\n",
              " 'swum': 987,\n",
              " 't': 988,\n",
              " 'table': 989,\n",
              " 'take': 990,\n",
              " 'taken': 991,\n",
              " 'talking': 992,\n",
              " 'tea': 993,\n",
              " 'tears': 994,\n",
              " 'technicalities': 995,\n",
              " 'technique': 996,\n",
              " 'tell': 997,\n",
              " 'tells': 998,\n",
              " 'tempting': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *- building tokenizer*"
      ],
      "metadata": {
        "id": "HAm1qj3LMA2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concept dict.items()\n",
        "d = {\n",
        "    'name' : 'Kunal',\n",
        "    'age' : 25,\n",
        "    'gender' : 'male'\n",
        "}\n",
        "d.items()"
      ],
      "metadata": {
        "id": "1g2pLqXQMLXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer class\n",
        "# will have encode method and decode method\n",
        "\n",
        "class SimpleTokenv1:\n",
        "\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab # <- this is -> ['apple':1, 'banana':2]\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()} # this is [1:'apple', 2:'banana'] # <- derived attribute\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [p.strip() for p in preprocessed if p.strip()] # if p.strip() removes blank\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join(self.int_to_str[i] for i in ids)\n",
        "    # # remove spaces before normal punctuation\n",
        "    text = re.sub(r'\\s+([,.:;?!()\"\\'\\]])', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "6acMaIGBE4T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenv1(vocab)\n",
        "\n",
        "text = \"I ,HAD always 'thought', Jack Gisburn, rather a, cheap!!\"\n",
        "\n",
        "\n",
        "k = tokenizer.encode(text)\n",
        "k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7zb1IiqFHQC",
        "outputId": "ac466801-7ec0-4b05-bf1f-e60ebe2a9ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53, 5, 44, 163, 2, 1021, 2, 5, 57, 38, 5, 835, 129, 5, 271, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U0OdzNUrIKCv",
        "outputId": "5f8b0706-b142-4c23-b42e-d72d4bf79e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I, HAD always' thought', Jack Gisburn, rather a, cheap!!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *- words not in vocab*"
      ],
      "metadata": {
        "id": "6X9nucrONN6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello My name is Kunal'\n",
        "# tokenizer.encode(text) # <-- it will throw error as some words not exist\n",
        "\n",
        "# this means we need to have large vocabulary to accomodate each word ->\n",
        "# one approch is to add context tokens other we will discuss later"
      ],
      "metadata": {
        "id": "_s4yoVfjNT5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context tokens\n",
        "#  <|unk|>    -> for unknown words in vocab\n",
        "# <|endoftext|>   -> for markers are end of text/document for seperation\n",
        "\n",
        "\n",
        "# depending on LLM researchers also considers\n",
        "# [BOS] -> begining of sequence -> start of text\n",
        "# [EOS] -> end of sequence\n",
        "# [PAD] -> padding -> short text padded upto longest text\n",
        "\n",
        "# GPT -> <|endoftext|> but not use <|unk|> instead use Byte pair Encoding"
      ],
      "metadata": {
        "id": "ooaIVC3POELH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_data.extend(['<|unk|>','<|endoftext|>'])"
      ],
      "metadata": {
        "id": "mx1bwoUHPJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_data[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d33YG4OUPYxm",
        "outputId": "ab1cd2da-f9f6-4d85-ef92-e54338cdf4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yourself', '<|unk|>', '<|endoftext|>', '<|unk|>', '<|endoftext|>']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the vocab\n",
        "vocab = {token:integer for integer,token in enumerate(sorted_data)} # use enemurate"
      ],
      "metadata": {
        "id": "XNnIQzTDPem1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(vocab.items())[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjdOfhXePzv9",
        "outputId": "25a01535-1b68-485b-dea0-2bd7dfccdddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('younger', 1145),\n",
              " ('your', 1146),\n",
              " ('yourself', 1147),\n",
              " ('<|unk|>', 1150),\n",
              " ('<|endoftext|>', 1151)]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will modify our token generator ->\n",
        "\n",
        "class SimpleTokenv2:\n",
        "\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab # <- this is -> ['apple':1, 'banana':2]\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()} # this is [1:'apple', 2:'banana'] # <- derived attribute\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [p.strip() for p in preprocessed if p.strip()] # if p.strip() removes blank\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed] # <- updated\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join(self.int_to_str[i] for i in ids)\n",
        "    # remove spaces before normal punctuation\n",
        "    text = re.sub(r'\\s+([,.:;?!()\"\\'\\]])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "ndxZ3CK3QEWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello My name is Kunal'\n",
        "\n",
        "tokenizer = SimpleTokenv2(vocab)\n",
        "k = tokenizer.encode(text)\n",
        "k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBgSvN6aRU8z",
        "outputId": "1b874256-2de0-4137-f803-9d2662f7f57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1150, 68, 1150, 598, 1150]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oT69V7jPR_Ez",
        "outputId": "7d71b2aa-f894-4c64-ccda-8cc328d6623e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> My <|unk|> is <|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Hello do you like Tea?'\n",
        "text2 = 'my name is kunal'\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1,text2))\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZKz1oW0CSIKi",
        "outputId": "cbcf6d2f-a42e-4afc-8f93-0618d80cd483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello do you like Tea? <|endoftext|> my name is kunal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = tokenizer.encode(text)\n",
        "k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBrr6sv1ShCi",
        "outputId": "7e37d70b-eadc-4260-e78a-9fa69056f635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1150, 374, 1144, 640, 1150, 10, 1151, 711, 1150, 598, 1150]"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "q0cQrFIqSluS",
        "outputId": "e270e4e2-d222-4f7a-dc65-ecc970ceaf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> do you like <|unk|>? <|endoftext|> my <|unk|> is <|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *- Summery*"
      ],
      "metadata": {
        "id": "Y74GUNgNeDUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte pair encoding"
      ],
      "metadata": {
        "id": "hCvcGbuCD4YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use to train GPT-2, GPT-3"
      ],
      "metadata": {
        "id": "qDiZ6Z01eC6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# algorithms\n",
        "\n",
        "'''\n",
        "1. word based -> every word is token\n",
        "\n",
        "- my hobby is to play cricket => ['my','hobby','is','to','play','cricket']\n",
        "-> out of vocab words -> boy and boys are different and simialirity is not captured\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "2/ character based -> each character is token\n",
        "\n",
        "- my name is kunal -> ['','','','','','','',...]\n",
        "-> small vocab size and out of vocab will not problem\n",
        "-> meaning associated is lost\n",
        "-> tokenized sequence is much longer than raw text -> text -1 & token-more\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "3/ subword based ->\n",
        "\n",
        "rule1 = donot split frequently used words into smaller subwords\n",
        "rule2 = split rare words into smaller, meaning subwords -> required -> dropdown to character\n",
        "\n",
        "boy and boys -> boy should not split and boys -> boy + s\n",
        "\n",
        "\n",
        "-> helps model understand that different word with same root word are similar in meaning\n",
        "--> token, tokens, tokinizing are all same\n",
        "\n",
        "-> helps model to understand that tokinization and modernization are made up of\n",
        "differnt root word but have same suffix 'ization' and used in same syntactic situations\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "8V9skQZYEKoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE is subword tokenization algorithm\n",
        "# introduced 1994\n",
        "\n",
        "# https://www.semanticscholar.org/paper/A-new-algorithm-for-data-compression-Gage/1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8\n",
        "\n",
        "'''\n",
        "- most common pair of consecutive byte(2) of data is replace it with byte\n",
        "that doesnot occur in data\n",
        "\n",
        "aaabdaaabac => aa with z\n",
        "zabdzabac => ab with y\n",
        "zydzyac => compressed data => we can go another layer -> zy to w\n",
        "wdwc\n",
        " '''\n"
      ],
      "metadata": {
        "id": "nU7iQxhtF2du",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "de1ca4c8-539f-401a-9f12-dfad0f40e952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n- most common pair of consecutive byte(2) of data is replace it with byte\\nthat doesnot occur in data\\n\\naaabdaaabac => aa with z\\nzabdzabac => ab with y\\nzydzyac => compressed data => we can go another layer -> zy to w\\nwdwc\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE ->\n",
        "# 1/ most common words in vocab is represented as single word\n",
        "# 2/ rare words are broken down into subwords"
      ],
      "metadata": {
        "id": "22jttMP3KYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing -\n",
        "'''\n",
        "- old 7, older 3, finest 9, lowest 4\n",
        "#---------------------------------------------------\n",
        "preprocessing -> each token with ending </w>\n",
        "\n",
        "- old</w> 7, older</w> 3, finest</w> 9, lowest</w> 4\n",
        "#---------------------------------------------------\n",
        "setep/1 -> split words into characters and make frequency table\n",
        "\n",
        "</w> -> 23\n",
        "o -> 14\n",
        "l -> 14\n",
        "d -> 10\n",
        "e -> 16\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "s -> 13\n",
        "t -> 13\n",
        "w -> 4\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "step/2 -> most frequent character -> e check it's pairing\n",
        "\n",
        "er -> 3\n",
        "es -> 13\n",
        "\n",
        "# merge these tokens\n",
        "\n",
        "</w> -> 23\n",
        "o -> 14\n",
        "l -> 14\n",
        "d -> 10\n",
        "e -> 16 ==> remove 13 -> 3\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "s -> 13 ==> remove 13 -> 0 => s always appeares with e\n",
        "t -> 13\n",
        "w -> 4\n",
        "es -> 13\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#setp/3 -> check if this new token merges with something or not\n",
        "\n",
        "est -> 13\n",
        "\n",
        "# merge these tokens\n",
        "\n",
        "</w> -> 23\n",
        "o -> 14\n",
        "l -> 14\n",
        "d -> 10\n",
        "e -> 3\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "t -> 13 ==> remove 13 -> 0\n",
        "w -> 4\n",
        "es -> 13 ==> remove 13 -> 0\n",
        "est -> 13 # <-- common root word\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "step/4\n",
        "\n",
        "est</w> -> 13  -> important to understand the position of words\n",
        "\n",
        "merge this token\n",
        "\n",
        "</w> -> 23 ==> remove 13 -> 10\n",
        "o -> 14\n",
        "l -> 14\n",
        "d -> 10\n",
        "e -> 3\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "w -> 4\n",
        "est -> 13 ==> remove 13 -> 0\n",
        "est</w> -> 13  -> gets information as est</w> always comes at end -> ending sequence -> statement ends here\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "step/5 ol pair occurs most\n",
        "\n",
        "ol -> 10 times\n",
        "\n",
        "\n",
        "</w> -> 10\n",
        "o -> 14 ==> remove 10 -> 4\n",
        "l -> 14 ==> remove 10 -> 4\n",
        "d -> 10\n",
        "e -> 3\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "w -> 4\n",
        "est</w> -> 13\n",
        "ol -> 10\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "step/6\n",
        "\n",
        "old -> 10\n",
        "\n",
        "</w> -> 10\n",
        "o -> 4\n",
        "l -> 4\n",
        "d -> 10 ==> remove 10 -> 0\n",
        "e -> 3\n",
        "r -> 3\n",
        "f -> 9\n",
        "i -> 9\n",
        "n -> 9\n",
        "w -> 4\n",
        "est</w> -> 13\n",
        "ol -> 10 ==> remove 10 -> 0\n",
        "old -> 10 -> captured root word\n",
        "\n",
        "\n",
        "-> here f,i,n also exist but not in diverse so we will not modify that\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# new generated vocab ->\n",
        "\n",
        "est</w>\n",
        "old\n",
        "</w>\n",
        "o\n",
        "l\n",
        "e\n",
        "r\n",
        "f\n",
        "i\n",
        "n\n",
        "w\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# stopping criteria will either be token count or number of iterations\n",
        "\n",
        "-> solves out of vocabulary problems and root meanings are captured\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "BHWRA2QoLceX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *- bite pair incoder for openAI models*\n",
        "\n"
      ],
      "metadata": {
        "id": "aqn-Ia8ffyyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq2Djb65fyNr",
        "outputId": "58d7b0a0-ebd0-425f-c3c6-d716196910bd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "importlib.metadata.version('tiktoken') # <-- version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eYoAeuKYfwF7",
        "outputId": "376bab1d-4bec-40f8-ac03-eb0a458775a1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2') # <-- get the vocab and encoding for gpt2\n",
        "# same as our simpletokenizer class -> encode and decode method"
      ],
      "metadata": {
        "id": "SjwaOtycgPok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello my name is Kunal <|endoftext|> and I like someunknownfeeling'\n",
        "encoded_text = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "encoded_text\n",
        "# someunknownfeeling -> split into subwords\n",
        "# token 50257 -> token limit of gpt2 -> english has 170000-2000000 words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_jLQCEHgcpD",
        "outputId": "ab1a565c-6b45-4ab9-ab1c-fee7ae231cf5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15496,\n",
              " 616,\n",
              " 1438,\n",
              " 318,\n",
              " 509,\n",
              " 18835,\n",
              " 220,\n",
              " 50256,\n",
              " 290,\n",
              " 314,\n",
              " 588,\n",
              " 617,\n",
              " 34680,\n",
              " 5036,\n",
              " 10809]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "decoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YhO8TEJ8kgd8",
        "outputId": "9bf1f0fe-e53c-4901-9549-2202a1ad6c8b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello my name is Kunal <|endoftext|> and I like someunknownfeeling'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dealing with unknown tokens"
      ],
      "metadata": {
        "id": "ujDKlKJTlfKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unk_tok = tokenizer.encode('Akdfffdghlkr erstrsd') #<- does BPE\n",
        "print(unk_tok)\n",
        "\n",
        "dec_unk = tokenizer.decode(unk_tok)\n",
        "print(dec_unk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRe3u2dBlkF9",
        "outputId": "c605f596-641f-4450-c6b3-221bafdfe69d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 67, 487, 16344, 456, 75, 38584, 1931, 2536, 21282]\n",
            "Akdfffdghlkr erstrsd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizers and their use case"
      ],
      "metadata": {
        "id": "c5dabn9wmxVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will consist of types of tokenizers used in different kind of language models and comparision"
      ],
      "metadata": {
        "id": "PoBiJ9hGmzwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#--"
      ],
      "metadata": {
        "id": "T6kRtPPQYT3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *input - target pairs*"
      ],
      "metadata": {
        "id": "Qw-jwuBaYNv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# autoregressive model\n",
        "\n",
        "'''\n",
        "model predicts one word at a time ->\n",
        "\n",
        "This\n",
        "This is                           <- this become input to next iteration\n",
        "This is some                      <- 'some' is predicted\n",
        "This is some facinating\n",
        "this is some facinating stuff\n",
        "\n",
        "# this is self supervised learning and auto_regressive\n",
        "\n",
        " '''\n",
        " # we will mask out all the words which is suppose to be predicted"
      ],
      "metadata": {
        "id": "6A1Xg1xPYSPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/4. Data Files/LLM Dataset/the verdict.txt','r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "J1SQRiEPZbo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "bEFXh6_UZ1a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "fMMivl-IZ4V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer.encode(raw_text)\n",
        "print(len(encoded_text)) # <- vocab size\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GbELv6_aFeb",
        "outputId": "71ee4212-34c3-4d3d-fc64-fb91fd1bc390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5147\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526, 628, 198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context size -> how many words as input to get the output"
      ],
      "metadata": {
        "id": "TCXmVqyidtmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sample = encoded_text[50:]"
      ],
      "metadata": {
        "id": "QHWDO8U4dQNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "# model is trained to look for 4 words and then predict\n",
        "# how much words model should pay attention at one time\n",
        "'''\n",
        "x = [1,2,3,4]\n",
        "y = [2,3,4,5]\n",
        "\n",
        "1 -> 2\n",
        "12 -> 3\n",
        "123 -> 4\n",
        "1234 -> 5\n",
        "\n",
        " '''\n",
        "\n",
        "x = encoded_sample[:context_size]\n",
        "y = encoded_sample[1:context_size+1]\n",
        "\n",
        "print('x',x)\n",
        "print('y',y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGfJtERid2_1",
        "outputId": "1b7b7bee-feeb-409b-eb23-4fbce37ad960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x [290, 4920, 2241, 287]\n",
            "y [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "\n",
        "  input = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "\n",
        "  print(f'{input} --> {desired}')\n",
        "  print(f'{tokenizer.decode(input)} --> {tokenizer.decode([desired])}')\n",
        "  print('-'*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuewh7uHewTQ",
        "outputId": "eb0b5eb2-2de3-4d60-b847-1631f4562074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] --> 4920\n",
            " and -->  established\n",
            "--------------------------------------------------\n",
            "[290, 4920] --> 2241\n",
            " and established -->  himself\n",
            "--------------------------------------------------\n",
            "[290, 4920, 2241] --> 287\n",
            " and established himself -->  in\n",
            "--------------------------------------------------\n",
            "[290, 4920, 2241, 287] --> 257\n",
            " and established himself in -->  a\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset and dataloader in pytorch"
      ],
      "metadata": {
        "id": "6y8p3pSFg24K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to convert it into more structured format and in tensor (pytorch)\n",
        "# -> tensor -> multidimensional array\n",
        "'''\n",
        "1 -> input tensor which llm sees\n",
        "2 -> output tensor for llm to predict\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "6LhWdplcffGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6610fa2f-8530-481a-c0a3-69264c3741ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1 -> input tensor which llm sees\\n2 -> output tensor for llm to predict\\n\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use pytorch's built-in dataset and dataloader classes\n",
        "\n",
        "'''\n",
        "fetches the input output target pairs using sliding window approch -> slide the input\n",
        "\n",
        "we collect inputs in tensor x where each row represents input context\n",
        "\n",
        "x = tensor([\n",
        "  ['in','the','heart','of'],        #<-- each similar row is input -> has 4 prediction tasks -> context is 4\n",
        "  ['the','city','stood','the'],\n",
        "  ['old','library',',','a']\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "y = tensor([\n",
        "  ['the','heart','of','the'],   #<-- each similar row is output ->\n",
        "  ['city','stood','the','old'],\n",
        "  ['library',',','a','relitic']\n",
        "])\n",
        "\n",
        "\n",
        " '''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uMgqSifmgw-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# will be done in 4 steps\n",
        "'''\n",
        "1/ tokenize entire text\n",
        "2/ using sliding window to chunk the book into overlapping sequence of max length\n",
        "3/ return total numbers of rows in dataset\n",
        "4/ return single row in dataset\n",
        "\n",
        " '''\n",
        "\n",
        "'''\n",
        " below is the Dataset class\n",
        " #-> defines how rows are fetched from dataset\n",
        " #->\n",
        "\n",
        "  '''"
      ],
      "metadata": {
        "id": "S5mRG97Ji97E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "55183091-5d7f-4ca5-de50-b0b2dc0eeb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n below is the Dataset class\\n #-> defines how rows are fetched from dataset \\n #->\\n\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "  def __init__(self,text,tokenizer,max_length,stride):\n",
        "    self.input_ids =[]\n",
        "    self.target_ids = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_ids = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    # use sliding window approch to chunk book into overlapping sequence of max_length\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride): # <- range(start,end,jump)\n",
        "\n",
        "      input_chunk = token_ids[i:i+max_length] # <-- getting chunks one by one by index\n",
        "      target_chunk = token_ids[i+1 : i+max_length+1]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk)) #<-- converting those chunks to pytorch chunks\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "#-------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx): # <-- dataloder requires this method <- to fetch one by one\n",
        "    return self.input_ids[idx],self.target_ids[idx] #<-- returns the id's of tensors\n"
      ],
      "metadata": {
        "id": "24y1PwrAjZol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k= torch.tensor([1,2,3,4])\n",
        "l = torch.tensor([2,3,4,5])\n",
        "print(k)\n",
        "print(l)\n",
        "\n",
        "t = []\n",
        "\n",
        "t.append(k)\n",
        "t.append(l)\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjIfhtYiCiID",
        "outputId": "719a6689-5495-4da0-bcec-a4a9db69fec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "tensor([2, 3, 4, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 2, 3, 4]), tensor([2, 3, 4, 5])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# following code will use the GPTDatasetV1 to load the inputs in batches via PyTorch DataLoader"
      ],
      "metadata": {
        "id": "oLmt7vC-E8HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# steps ->\n",
        "\n",
        "1/ initialize dataset\n",
        "2/ create dataset\n",
        "3/ drop_last = True. Drops the last batch if shorter than specified batch_size to prevent spikes during training\n",
        "4/ The number of CPU processes to use for preprocessing\n",
        "\n",
        "- helps for parallel processing\n",
        "- analyze multiple batches at same time\n",
        "\n",
        "batch_size => number of batches model process at once before updating it's parameters\n",
        "num_workers => parallel processing\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "g46ZdbC9GDNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crate_dataloader_v1(txt,batch_size=4,max_length=256,\n",
        "                        stride=128, shuffle=True, drop_last=True,\n",
        "                        num_workers=0):\n",
        "\n",
        "  #initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "  # Create Dataset\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  # crate dataloder\n",
        "  dataloder = DataLoader(\n",
        "      dataset,  # <-- dataset contains training and test tensors\n",
        "      batch_size=batch_size, # <-- size of tensors\n",
        "      shuffle=shuffle, # <-- True\n",
        "      drop_last=drop_last, # <-- True\n",
        "      num_workers=num_workers # <-- 0\n",
        "  )\n",
        "\n",
        "  return dataloder\n"
      ],
      "metadata": {
        "id": "hT4VxwONGhSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # testing dataloader with batch size 1 and context of 4"
      ],
      "metadata": {
        "id": "4Y6sQd0AJiAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/4. Data Files/LLM Dataset/the verdict.txt','r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "MDJHr0EVMLgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the dataloader into python iterator to fetch next entry via python's next function"
      ],
      "metadata": {
        "id": "-7YdZq8AMewJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dataloader = crate_dataloader_v1(raw_text,batch_size=1,max_length=4,\n",
        "                                 stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # <-- get input tensor and output tensor\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch) # <-- stride = 1 means 1 shifting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLMPvMi4Mqyw",
        "outputId": "44768d9b-df07-455b-fa83-fa4ec4e1686e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as max length is set to 4 -> each tensors stores 4 token id's\n",
        "# in gpt it usually takes 256\n",
        "\n",
        "'''\n",
        "# -> batch size 1 is for illustration purposes -> small batches requires less memory\n",
        "but lead to more noicy model updates\n",
        "--> batch size is tradeoff and need to set a good parameter (hyperparameter)\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "8MJ1Z1qwNwTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size more than 1\n",
        "\n",
        "dataloader = crate_dataloader_v1(raw_text,batch_size=8,max_length=4,\n",
        "                                 stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)\n",
        "\n",
        "# input tensor has 8 inputs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "igCFMBPyOF3q",
        "outputId": "277543a6-c68a-416f-bf0d-8ff82cffc6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   40,   367,  2885,  1464],\n",
            "        [  367,  2885,  1464,  1807],\n",
            "        [ 2885,  1464,  1807,  3619],\n",
            "        [ 1464,  1807,  3619,   402],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  402,   271, 10899,  2138],\n",
            "        [  271, 10899,  2138,   257]]), tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 2885,  1464,  1807,  3619],\n",
            "        [ 1464,  1807,  3619,   402],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  402,   271, 10899,  2138],\n",
            "        [  271, 10899,  2138,   257],\n",
            "        [10899,  2138,   257,  7026]])]\n",
            "[tensor([[10899,  2138,   257,  7026],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [ 7026, 15632,   438,  2016],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 2016,   257,   922,  5891],\n",
            "        [  257,   922,  5891,  1576]]), tensor([[ 2138,   257,  7026, 15632],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [ 7026, 15632,   438,  2016],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 2016,   257,   922,  5891],\n",
            "        [  257,   922,  5891,  1576],\n",
            "        [  922,  5891,  1576,   438]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# we increase stride to 4 to utilize data fully(we don't skip a single word)\n",
        "but also provide any overlap between batches since more overlap could lead to\n",
        "increase overfitting\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "ArYn9DUUPSQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#--"
      ],
      "metadata": {
        "id": "W78rQLsaZdfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Embeddings"
      ],
      "metadata": {
        "id": "ZtdIfwDEZYGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector embedding\n",
        "# not word embedding -> can include subwords"
      ],
      "metadata": {
        "id": "jLBd5dY-Zcpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Representing words numerically ->\n",
        "\n",
        "1/ random numbers -> will not get the true reletionships between numbers\n",
        "\n",
        "CNN -> encode special reletion between the pixels to predict -> exploit the info.\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "2/ one hot encoding -> fails to get symentic reletionship 1000000100010001000100\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "3/ vector encoding -> every word is vector\n",
        "-> dimension is determined by feature\n",
        "\n",
        "\n",
        "# Features: [has_a_tail, is_eatable, has_4_legs, makes_sound, is_a_pet]\n",
        "\n",
        "dog     = [23,  2, 19, 12, 35]\n",
        "cat     = [31,  3, 21, 18, 31]\n",
        "apple   = [1,  22,  0, 0.5, 5]\n",
        "banana  = [2,  38,  0, 0.2, 7]\n",
        "\n",
        "for similar features both dog and cat are closer and same for apple and banana\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "stM-cwI7Z1fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to make this vector embedding ?\n",
        "\n",
        "'''\n",
        "we need to train neural network for this\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "U1Ei11oZchK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *demo*"
      ],
      "metadata": {
        "id": "As1I67GhdVg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demo ->\n",
        "\n",
        "# many companies has already a pre trained model like - word2vec https://huggingface.co/fse/word2vec-google-news-300\n",
        "# 300 is dimension\n"
      ],
      "metadata": {
        "id": "g9eAugT-c22O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KpCiPbM6dX3X",
        "outputId": "1dbbf98d-4dee-45ae-88ad-ea2d6b730e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6cXAoqAdjrt",
        "outputId": "9fc63c36-7e20-4064-eb8c-851bfb4627bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model # <--- it is basically dictionary with words and their vectors"
      ],
      "metadata": {
        "id": "huSvJnbpd1T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors['man'] #<-- indexing dict item"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "COiwHoNIeR-1",
        "outputId": "0d65cbf3-403f-4730-933f-09faa8797c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
              "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
              "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
              "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
              "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
              "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
              "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
              "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
              "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
              "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
              "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
              "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
              "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
              "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
              "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
              "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
              "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
              "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
              "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
              "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
              "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
              "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
              "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
              "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
              "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
              "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
              "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
              "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
              "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
              "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
              "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
              "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
              "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
              "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
              "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
              "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
              "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
              "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
              "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
              "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
              "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
              "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
              "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
              "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
              "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
              "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
              "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
              "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
              "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
              "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
              "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
              "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
              "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
              "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
              "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
              "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
              "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
              "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
              "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
              "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similar words ->\n",
        "word_vectors.most_similar(positive=['father','daughter'],negative=['son'],topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZUBEqqsfdQD",
        "outputId": "3ab174fc-328a-4561-fa15-1d4f7c520063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mother', 0.8948175311088562),\n",
              " ('niece', 0.8004239201545715),\n",
              " ('eldest_daughter', 0.7821366786956787),\n",
              " ('grandmother', 0.780648410320282),\n",
              " ('aunt', 0.7747992277145386),\n",
              " ('granddaughter', 0.771183967590332),\n",
              " ('sister', 0.7706005573272705),\n",
              " ('husband', 0.7571515440940857),\n",
              " ('wife', 0.7549542784690857),\n",
              " ('daughters', 0.7524171471595764)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of calculating similarity\n",
        "print(word_vectors.similarity('woman', 'man'),'# <-- high relation')\n",
        "print(word_vectors.similarity('king', 'queen'),'# <-- high relation')\n",
        "print(word_vectors.similarity('Instagram', 'Facebook'),'# <-- high relation')\n",
        "print(word_vectors.similarity('boy', 'girl'),'# <-- high relation')\n",
        "print(word_vectors.similarity('rock', 'tree'),'# <-- low relation')\n",
        "print(word_vectors.similarity('paper', 'water'),'# <-- low relation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5E6VUhPe1wY",
        "outputId": "18db81d3-9e7e-405d-d73b-eadd84c217e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.76640123 # <-- high relation\n",
            "0.6510957 # <-- high relation\n",
            "0.55903774 # <-- high relation\n",
            "0.8543272 # <-- high relation\n",
            "0.1536612 # <-- low relation\n",
            "0.11408084 # <-- low relation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most similar words\n",
        "\n",
        "word_vectors.most_similar('Ronaldo',topn=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGg77MgYg8I7",
        "outputId": "b8332bfc-2df2-4c24-8786-a7eb451b7181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Cristiano_Ronaldo', 0.8814299702644348),\n",
              " ('Ronaldinho', 0.8581245541572571),\n",
              " ('Robinho', 0.8249523043632507),\n",
              " ('Messi', 0.8209547996520996)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so basically vector encodes the meaning"
      ],
      "metadata": {
        "id": "2To9T_Lrh5wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how token embedding are created in LLM"
      ],
      "metadata": {
        "id": "2QwVkxQmh_4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # gpt 2 -> vocab = 50257 and dimension"
      ],
      "metadata": {
        "id": "CmEhuejgiDGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Vocabulary size (rows) and vector dimension (columns)\n",
        "vocab_size = 10      # small example instead of 50257\n",
        "embed_dim  = 30       # small example instead of 768\n",
        "\n",
        "# Create a random \"embedding matrix\"\n",
        "E = np.random.randn(vocab_size, embed_dim)\n",
        "\n",
        "print(\"Embedding matrix shape:\", E.shape)\n",
        "print(\"Rows = token IDs, Columns = vector dimensions\\n\")\n",
        "\n",
        "# Print matrix with headers\n",
        "header = [\"tok_id\"] + [f\"d{j}\" for j in range(embed_dim)]\n",
        "print(\"  \".join(f\"{h:>6}\" for h in header))\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    row_vals = \"  \".join(f\"{v:6.2f}\" for v in E[i])\n",
        "    print(f\"{i:6d}  {row_vals}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OG7ohRhlAEP",
        "outputId": "d95b6fa5-a700-4bde-c265-8f6fd9f6f727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (10, 30)\n",
            "Rows = token IDs, Columns = vector dimensions\n",
            "\n",
            "tok_id      d0      d1      d2      d3      d4      d5      d6      d7      d8      d9     d10     d11     d12     d13     d14     d15     d16     d17     d18     d19     d20     d21     d22     d23     d24     d25     d26     d27     d28     d29\n",
            "     0   -0.72   -0.56   -0.84   -0.83   -0.62    0.19    0.87    0.46   -0.33    2.05   -1.02    0.63   -0.15    0.29   -0.94    2.58    2.02    0.42    0.33   -0.26    2.21    0.64   -0.87    0.51    1.95    0.19   -1.28    0.06    1.06   -2.49\n",
            "     1   -0.06   -0.93   -0.27   -1.67    0.05   -0.64    0.90   -0.02    0.33   -1.15    0.10   -1.74   -0.23    0.43    0.96   -1.12    1.14    1.12   -1.97    0.02   -0.62    0.98    0.06   -1.30   -2.85   -1.85   -0.79    0.32    1.73   -0.51\n",
            "     2   -0.94    1.61   -0.10    1.04    0.31    0.02   -1.75    0.22   -2.07   -0.07   -0.84    0.51    0.22   -1.62    2.26    0.02   -0.45    1.46    1.01   -0.17    1.13   -0.52   -1.20   -0.56   -0.02   -1.13    1.03    1.31   -0.88    1.95\n",
            "     3   -0.47   -0.88   -1.43   -1.28    0.62   -0.28   -1.29    0.17    1.39    0.10    0.33    0.17   -0.37   -0.68   -1.38   -1.25    0.28    0.16    0.35   -1.36   -0.12    0.29    0.16   -0.83    1.98    0.96   -2.74    1.52    1.10    0.03\n",
            "     4   -2.45    0.15   -0.30    0.40   -0.15    0.86    0.05   -0.04   -0.97    0.92    1.78    1.40   -0.19    0.11   -0.81   -0.81    0.91    0.84   -0.80   -1.34    0.59   -0.89   -0.20    1.35    2.33    0.09   -1.20   -0.46   -1.13   -1.12\n",
            "     5   -0.34   -1.81   -0.04    0.94   -0.20   -1.03   -0.12   -1.71    0.87   -0.44   -1.32    0.05   -0.56    0.03   -2.22    0.93    1.04   -0.34    0.17   -2.60   -1.26   -1.16    0.52   -0.62    0.21    1.96    0.88    1.52   -1.61   -0.79\n",
            "     6   -0.74   -0.29   -1.31    2.26    1.05    0.97    0.01    0.45    1.10    1.02    0.24    0.24    0.61   -0.08    2.23    0.85   -0.10    0.99    0.65    0.01    0.35    1.04    0.87    1.45    0.85   -0.54   -0.48   -0.04    0.34    0.18\n",
            "     7    2.39    1.49   -1.23    0.33    1.17    3.06   -0.46    0.52    1.47    0.73    0.02   -0.25    0.16    0.39   -0.38    0.29   -0.44    1.01    0.31    0.21   -2.07   -1.18   -0.30    1.13   -0.15    0.43   -1.35   -0.47   -0.64    0.51\n",
            "     8    0.91    1.34    1.15    0.80   -0.73    0.59   -0.51   -2.66    0.91   -1.86   -2.69    1.51    0.96    1.07    0.28    0.73   -0.98    2.49    0.07   -0.01    0.21    1.19   -1.13   -2.90    0.54   -0.72   -0.64   -0.34    0.29    2.31\n",
            "     9    0.98    0.42   -0.99    0.77    1.23    0.24    1.21   -0.39   -0.44   -0.24   -1.37    0.52   -0.43   -0.30   -0.82    1.12   -1.35   -0.03   -0.24    0.29   -0.69   -0.25    0.64    0.63    1.56   -0.39    0.65   -1.60   -1.10   -1.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "-1/ we initialize embedding weights with random values\n",
        "-2/ initialization will work as a starting point for LLM learning\n",
        "-3/ embedding weights are then optimized as part of LLM training\n",
        "\n",
        "---> based on training data\n",
        "---> backpropogation is implemented\n",
        "\n",
        "=> so basically we perform 2 training one for GPT and other for embedding matrix\n",
        "\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "4bpgIwESlEru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initialization for embedding matrix"
      ],
      "metadata": {
        "id": "SX51xpQ8mBHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Z3x7fykuRMhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplocity we will use small vocab of 6 tokens and embedding of size 3\n",
        "'''\n",
        "for each token we will have 3 dimensional vector\n",
        "'''\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim) # why called lookup table?\n",
        "embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmA_s6D4Q_YR",
        "outputId": "24a933a4-0a88-4101-e154-530bf78ec160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(6, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight # initialized the weights randomly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmfaTjdDR287",
        "outputId": "d19c3a76-cecc-4fbe-9e36-711ce3312d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3374, -0.1778, -0.1690],\n",
              "        [ 0.9178,  1.5810,  1.3010],\n",
              "        [ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-1.1589,  0.3255, -0.6315],\n",
              "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consist of small random values which later then optimized via LLM training\n",
        "\n",
        "# to get vector for id_level 3 ->\n",
        "\n",
        "embedding_layer(torch.tensor([3])) # <- ecactly like dictionary lookup #<- lookup operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3DEOtUJSIgT",
        "outputId": "9216b08c-91e8-4b08-f782-d11dc9c9f234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2,3,5,1])\n",
        "embedding_layer(input_ids) # <-- returns the embedding matric for those IDs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85JBiTDHOnWZ",
        "outputId": "84d3565a-6c5e-4b24-9f07-d9f3251f1fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-2.8400, -0.7849, -1.4096],\n",
              "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding layer is same as neural network linear layer\n",
        "\n",
        "# -> embedding layer is computationally efficient since NN linear layer has many unnecessary multiplication\n",
        "#___ with zero\n",
        "\n",
        "# torch.nn.linear() #<- not efficient"
      ],
      "metadata": {
        "id": "M_nsaO9CP_im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#---"
      ],
      "metadata": {
        "id": "ghab1OdCRoUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding"
      ],
      "metadata": {
        "id": "13gp1j-vRllY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "- till now we have learned how to convert words into vectors\n",
        "but in statements positions also matters the most apart from symentic meaning\n",
        "\n",
        "-> vector embedding -> symentic meaning\n",
        "-> positional embedding -> reletion between positions\n",
        "\n",
        "\n",
        " '''"
      ],
      "metadata": {
        "id": "R2NJFSj0RpkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cat sat on the mat\n",
        "# on the mat cat sat\n",
        "\n",
        "'''\n",
        "same token ID gets mapped to same vector representation regardless of where the\n",
        "token is positioned\n",
        "\n",
        "-> position of cat -> embedded is same\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Vw7mA6aNRn1y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Bl6UQWfKSLMK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([1,2,2,1])"
      ],
      "metadata": {
        "id": "t3rvw1NUSuyQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_embedding = torch.nn.Embedding(5,4)\n",
        "vector_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19LlxpOhS0FR",
        "outputId": "8f5d2054-907f-45c0-c90d-bb040fd85609"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(5, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_embedding(input) # <-- is not taking leverage of position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFEQnp5tUNYT",
        "outputId": "75a2f28e-d5dd-411b-8dee-a5faf2bdd334"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5851, -1.0437, -0.6092,  2.7983],\n",
              "        [-0.9313, -0.2746, -0.3906, -0.3042],\n",
              "        [-0.9313, -0.2746, -0.3906, -0.3042],\n",
              "        [ 0.5851, -1.0437, -0.6092,  2.7983]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There are 2 types of positional embeddings\n",
        "\n",
        "'''\n",
        "1/ ABSOLUTE ->\n",
        "\n",
        "- commonly used\n",
        "-> for each positio in input sequence, a unique embedding is added to token emb\n",
        "\n",
        "x -> cat => x + y(positional) => x+y (result)\n",
        "x -> cat => x + z(positional) => x+z (result)\n",
        "\n",
        "so final embedding is different -> and encodes the position -> same dimension\n",
        "\n",
        "\n",
        "-> when fix order of tokens is crucial => sequence generation -> gpt trained\n",
        "and original transformer papaer is trained on this\n",
        "\n",
        "\n",
        "================================================================================\n",
        "\n",
        "2/ RELATIVE ->\n",
        "\n",
        "- emphasis is on the relative position or distance between tokens. learns the\n",
        "reletionships based on how far apart rather than at which exact position\n",
        "\n",
        "-> can genralized better to sequence of varying length\n",
        "=> sequences are very long\n",
        "\n",
        "\n",
        "-> language modelling over long sequences where same phrase can appear in different part of sequence\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "- makes LLM more aware about order and position\n",
        "- we need to optimize for token embedding + positional embedding\n",
        "\n",
        "=> in original paper they proposed the formula to add encoding but gpt optimized\n",
        "during training process\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "RNPo6ruXZlAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256  # gpt2 -> 768\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ],
      "metadata": {
        "id": "QEugs8HkdYT-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader and get sequence ->"
      ],
      "metadata": {
        "id": "Ca9CmBa8d-je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/4. Data Files/LLM Dataset/the verdict.txt','r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "w8IYWJ8Te9Nk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "  def __init__(self,text,tokenizer,max_length,stride):\n",
        "    self.input_ids =[]\n",
        "    self.target_ids = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_ids = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    # use sliding window approch to chunk book into overlapping sequence of max_length\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride): # <- range(start,end,jump)\n",
        "\n",
        "      input_chunk = token_ids[i:i+max_length] # <-- getting chunks one by one by index\n",
        "      target_chunk = token_ids[i+1 : i+max_length+1]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk)) #<-- converting those chunks to pytorch chunks\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "#-------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx): # <-- dataloder requires this method <- to fetch one by one\n",
        "    return self.input_ids[idx],self.target_ids[idx] #<-- returns the id's of tensors\n"
      ],
      "metadata": {
        "id": "-zn-ryYAeztP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,\n",
        "                        stride=128, shuffle=True, drop_last=True,\n",
        "                        num_workers=0):\n",
        "\n",
        "  #initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "  # Create Dataset\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  # crate dataloder\n",
        "  dataloder = DataLoader(\n",
        "      dataset,  # <-- dataset contains training and test tensors\n",
        "      batch_size=batch_size, # <-- size of tensors\n",
        "      shuffle=shuffle, # <-- True\n",
        "      drop_last=drop_last, # <-- True\n",
        "      num_workers=num_workers # <-- 0\n",
        "  )\n",
        "\n",
        "  return dataloder\n"
      ],
      "metadata": {
        "id": "XtTN3CfTe3GG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size = 8, max_length= max_length,\n",
        "    stride = max_length, shuffle= False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs,targets = next(data_iter)"
      ],
      "metadata": {
        "id": "p05kD_pie_2u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVauN2yJflx-",
        "outputId": "8f12eccc-2cfd-4eb7-f305-c468ca1ec06e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each token id we\n",
        "\n",
        "token_embedding_layer(inputs) # each input it is going to generate embedding for each values\n",
        "\n",
        "print(token_embedding_layer(inputs).shape) # this is the shape of"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOL9JSjygMz-",
        "outputId": "fb18203e-0853-4af3-84c2-f6335df1844b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# +------------------------------ width = 256 ------------------------------+\n",
        "# |     |     |     |     |     |     |    ...                            |\n",
        "# |     |     |     |     |     |     |                                    |\n",
        "# | 4 rows height                                                  (one block)\n",
        "# |                                                                          |\n",
        "# +--------------------------------------------------------------------------+\n",
        "#    <-------------------- one block --------------------->\n",
        "\n",
        "# Then 8 such blocks stacked in depth.\n"
      ],
      "metadata": {
        "id": "sBHfyqcQwfpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add positional encoding -> as only 4 tokens are given therefore need to encode 4 only\n"
      ],
      "metadata": {
        "id": "qvw5Zs7MhB2M"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}